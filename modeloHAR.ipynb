{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srdcf/HAR/blob/main/modeloHAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kJwioPUVH2Al",
        "outputId": "43438187-05c1-4150-dc12-27bf14b5c389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cTngiyQUIEpA"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tVn_qYp8xMNq"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('/content/drive/MyDrive/dataset1.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uEEpxzvOxj-q"
      },
      "outputs": [],
      "source": [
        "#df.drop('target', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Au_2Wybjf4oQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/drive/MyDrive/dataset2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "sTrsbVsY0kHw",
        "outputId": "ddce539c-0b06-44f5-abab-187f17692bd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0  Latitude Change  Longitude Change  Altitude Change  \\\n",
              "0                0              0.0               0.0              0.0   \n",
              "1                1              0.0               0.0              0.0   \n",
              "2                2              0.0               0.0              0.0   \n",
              "3                3              0.0               0.0              0.0   \n",
              "4                4              0.0               0.0              0.0   \n",
              "...            ...              ...               ...              ...   \n",
              "119860      119860              0.0               0.0              0.0   \n",
              "119861      119861              0.0               0.0              0.0   \n",
              "119862      119862              0.0               0.0              0.0   \n",
              "119863      119863              0.0               0.0              0.0   \n",
              "119864      119864              0.0               0.0              0.0   \n",
              "\n",
              "        Horizontal Accuracy Change  Speed Change  Gyroscope X  Gyroscope Y  \\\n",
              "0                              0.0           0.0    -0.063781     0.059786   \n",
              "1                              0.0           0.0    -0.042476     0.115178   \n",
              "2                              0.0           0.0     0.053395     0.148067   \n",
              "3                              0.0           0.0    -0.029693     0.114113   \n",
              "4                              0.0           0.0    -0.072303     0.098135   \n",
              "...                            ...           ...          ...          ...   \n",
              "119860                         0.0           0.0    -0.023435    -0.042742   \n",
              "119861                         0.0           0.0    -0.056324    -0.040612   \n",
              "119862                         0.0           0.0    -0.037150     0.020106   \n",
              "119863                         0.0           0.0     0.005326     0.062716   \n",
              "119864                         0.0           0.0     0.036218    -0.002264   \n",
              "\n",
              "        Gyroscope Z  Acceleration X Change  Acceleration Y Change  \\\n",
              "0          0.006924               0.000000               0.000000   \n",
              "1          0.101730               0.000000               0.000000   \n",
              "2          0.123034               0.000000               0.000000   \n",
              "3          0.088947               0.000000               0.000000   \n",
              "4          0.113447               0.000000               0.000000   \n",
              "...             ...                    ...                    ...   \n",
              "119860     0.042876               0.172020               0.885110   \n",
              "119861    -0.006125               0.453840               0.153720   \n",
              "119862    -0.095605              -0.634095              -0.175680   \n",
              "119863    -0.018908              -0.293410              -0.297985   \n",
              "119864     0.090811              -0.414800               0.690825   \n",
              "\n",
              "        Acceleration Z Change  Roll (ยบ)  Pitch (ยบ)   Yaw (ยบ)  \\\n",
              "0                    0.000000 -19.75767  -11.26569  0.944726   \n",
              "1                    0.000000 -19.66945  -11.03860  0.851585   \n",
              "2                    0.000000 -19.68774  -10.73308  0.631760   \n",
              "3                    0.000000 -19.80198  -10.73826  0.417986   \n",
              "4                    0.000000 -19.72018  -10.74029  0.188188   \n",
              "...                       ...       ...        ...       ...   \n",
              "119860              -0.186050   0.00000    0.00000  0.000000   \n",
              "119861              -0.135421   0.00000    0.00000  0.000000   \n",
              "119862              -0.034160   0.00000    0.00000  0.000000   \n",
              "119863              -0.127490   0.00000    0.00000  0.000000   \n",
              "119864              -0.132370   0.00000    0.00000  0.000000   \n",
              "\n",
              "        Magnetometer X Change  Magnetometer Y Change  Magnetometer Z Change  \\\n",
              "0                    0.000000                0.00000                0.00000   \n",
              "1                    0.000000                0.00000                0.00000   \n",
              "2                    0.000000                0.00000                0.00000   \n",
              "3                    0.000000                0.00000                0.00000   \n",
              "4                    0.000000                0.00000                0.00000   \n",
              "...                       ...                    ...                    ...   \n",
              "119860              -1.199999               -0.88125                0.80625   \n",
              "119861              -1.331250               -0.95625                0.91875   \n",
              "119862              -0.843749               -1.01250                0.50625   \n",
              "119863              -0.450000               -0.76875                0.31875   \n",
              "119864              -0.618751               -0.48750                0.18750   \n",
              "\n",
              "       DataLabel  \n",
              "0           idle  \n",
              "1           idle  \n",
              "2           idle  \n",
              "3           idle  \n",
              "4           idle  \n",
              "...          ...  \n",
              "119860      idle  \n",
              "119861      idle  \n",
              "119862      idle  \n",
              "119863      idle  \n",
              "119864      idle  \n",
              "\n",
              "[119865 rows x 19 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94f77db7-ee41-4887-9bad-d2ac512c35a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Latitude Change</th>\n",
              "      <th>Longitude Change</th>\n",
              "      <th>Altitude Change</th>\n",
              "      <th>Horizontal Accuracy Change</th>\n",
              "      <th>Speed Change</th>\n",
              "      <th>Gyroscope X</th>\n",
              "      <th>Gyroscope Y</th>\n",
              "      <th>Gyroscope Z</th>\n",
              "      <th>Acceleration X Change</th>\n",
              "      <th>Acceleration Y Change</th>\n",
              "      <th>Acceleration Z Change</th>\n",
              "      <th>Roll (ยบ)</th>\n",
              "      <th>Pitch (ยบ)</th>\n",
              "      <th>Yaw (ยบ)</th>\n",
              "      <th>Magnetometer X Change</th>\n",
              "      <th>Magnetometer Y Change</th>\n",
              "      <th>Magnetometer Z Change</th>\n",
              "      <th>DataLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.063781</td>\n",
              "      <td>0.059786</td>\n",
              "      <td>0.006924</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.75767</td>\n",
              "      <td>-11.26569</td>\n",
              "      <td>0.944726</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.042476</td>\n",
              "      <td>0.115178</td>\n",
              "      <td>0.101730</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.66945</td>\n",
              "      <td>-11.03860</td>\n",
              "      <td>0.851585</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053395</td>\n",
              "      <td>0.148067</td>\n",
              "      <td>0.123034</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.68774</td>\n",
              "      <td>-10.73308</td>\n",
              "      <td>0.631760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.029693</td>\n",
              "      <td>0.114113</td>\n",
              "      <td>0.088947</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.80198</td>\n",
              "      <td>-10.73826</td>\n",
              "      <td>0.417986</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.072303</td>\n",
              "      <td>0.098135</td>\n",
              "      <td>0.113447</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.72018</td>\n",
              "      <td>-10.74029</td>\n",
              "      <td>0.188188</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119860</th>\n",
              "      <td>119860</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.023435</td>\n",
              "      <td>-0.042742</td>\n",
              "      <td>0.042876</td>\n",
              "      <td>0.172020</td>\n",
              "      <td>0.885110</td>\n",
              "      <td>-0.186050</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.199999</td>\n",
              "      <td>-0.88125</td>\n",
              "      <td>0.80625</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119861</th>\n",
              "      <td>119861</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.056324</td>\n",
              "      <td>-0.040612</td>\n",
              "      <td>-0.006125</td>\n",
              "      <td>0.453840</td>\n",
              "      <td>0.153720</td>\n",
              "      <td>-0.135421</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.331250</td>\n",
              "      <td>-0.95625</td>\n",
              "      <td>0.91875</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119862</th>\n",
              "      <td>119862</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.037150</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>-0.095605</td>\n",
              "      <td>-0.634095</td>\n",
              "      <td>-0.175680</td>\n",
              "      <td>-0.034160</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.843749</td>\n",
              "      <td>-1.01250</td>\n",
              "      <td>0.50625</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119863</th>\n",
              "      <td>119863</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005326</td>\n",
              "      <td>0.062716</td>\n",
              "      <td>-0.018908</td>\n",
              "      <td>-0.293410</td>\n",
              "      <td>-0.297985</td>\n",
              "      <td>-0.127490</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.450000</td>\n",
              "      <td>-0.76875</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119864</th>\n",
              "      <td>119864</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036218</td>\n",
              "      <td>-0.002264</td>\n",
              "      <td>0.090811</td>\n",
              "      <td>-0.414800</td>\n",
              "      <td>0.690825</td>\n",
              "      <td>-0.132370</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.618751</td>\n",
              "      <td>-0.48750</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>idle</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>119865 rows ร 19 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94f77db7-ee41-4887-9bad-d2ac512c35a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94f77db7-ee41-4887-9bad-d2ac512c35a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94f77db7-ee41-4887-9bad-d2ac512c35a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-52bf9dba-862b-47bb-b674-3d2b99b4609a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52bf9dba-862b-47bb-b674-3d2b99b4609a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-52bf9dba-862b-47bb-b674-3d2b99b4609a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9ec2f965-1e72-4b57-b5fa-5906b6903337\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9ec2f965-1e72-4b57-b5fa-5906b6903337 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "IOrdeAtVZs9u"
      },
      "outputs": [],
      "source": [
        "# Encode categorical labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['DataLabelEncoder'] = label_encoder.fit_transform(df['DataLabel'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E85hyhFz0aW8",
        "outputId": "12f97def-5cef-4267-bc07-31938c282031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class to Label mapping:\n",
            "{'Moving': 0, 'idle': 1, 'picking': 2}\n"
          ]
        }
      ],
      "source": [
        "# Get the mapping of original classes to their corresponding numerical values\n",
        "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "print(\"Class to Label mapping:\")\n",
        "print(class_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JNyM-kdeO1y"
      },
      "outputs": [],
      "source": [
        "# Define the lag\n",
        "lag = 50\n",
        "\n",
        "# Compute change in GPS features with the specified lag\n",
        "df['Latitude Change'] = df['Latitude (ยบ)'].diff(periods=lag).fillna(0)\n",
        "df['Longitude Change'] = df['Longitude(ยบ)'].diff(periods=lag).fillna(0)\n",
        "df['Altitude Change'] = df['Altitude (m)'].diff(periods=lag).fillna(0)\n",
        "df['Horizontal Accuracy Change'] = df['Horizontal Accuracy (m)'].diff(periods=lag).fillna(0)\n",
        "df['Speed Change'] = df['Speed (m/s)'].diff(periods=lag).fillna(0)\n",
        "df['Acceleration X Change'] = df['Acceleration X (m.s2)'].diff(periods=lag).fillna(0)\n",
        "df['Acceleration Y Change'] = df['Acceleration Y (m.s2)'].diff(periods=lag).fillna(0)\n",
        "df['Acceleration Z Change'] = df['Acceleration Z (m.s2)'].diff(periods=lag).fillna(0)\n",
        "df['Magnetometer X Change'] = df['Magnetometer X (ut)'].diff(periods=lag).fillna(0)\n",
        "df['Magnetometer Y Change'] = df['Magnetometer Y (ut)'].diff(periods=lag).fillna(0)\n",
        "df['Magnetometer Z Change'] = df['Magnetometer Z (ut)'].diff(periods=lag).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aGbRmitxQ0h0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Split the data into features and target\n",
        "\n",
        "#.join(df.loc[:, 'Gyroscope X':'Gyroscope Z'])\n",
        "#.join(df.loc[:, 'Acceleration X (m.s2)':'Acceleration Z (m.s2)'])\n",
        "#.join(df.loc[:, 'Magnetometer X (ut)':'Magnetometer Z (ut)'])\n",
        "X = (df.loc[:, 'Latitude Change':'Speed Change']).join(df.loc[:, 'Gyroscope X':'Gyroscope Z']).join(df.loc[:, 'Acceleration X Change':'Acceleration Z Change']).join(df.loc[:, 'Roll (ยบ)':'Yaw (ยบ)']).join(df.loc[:, 'Magnetometer X Change':'Magnetometer Z Change'])\n",
        "\n",
        "y = df['DataLabelEncoder']  # Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "BamEPvYvNIac",
        "outputId": "89d0d097-7687-4d55-ff2d-086edd1ee301"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Latitude Change  Longitude Change  Altitude Change  \\\n",
              "0                   0.0               0.0              0.0   \n",
              "1                   0.0               0.0              0.0   \n",
              "2                   0.0               0.0              0.0   \n",
              "3                   0.0               0.0              0.0   \n",
              "4                   0.0               0.0              0.0   \n",
              "...                 ...               ...              ...   \n",
              "119860              0.0               0.0              0.0   \n",
              "119861              0.0               0.0              0.0   \n",
              "119862              0.0               0.0              0.0   \n",
              "119863              0.0               0.0              0.0   \n",
              "119864              0.0               0.0              0.0   \n",
              "\n",
              "        Horizontal Accuracy Change  Speed Change  Gyroscope X  Gyroscope Y  \\\n",
              "0                              0.0           0.0    -0.063781     0.059786   \n",
              "1                              0.0           0.0    -0.042476     0.115178   \n",
              "2                              0.0           0.0     0.053395     0.148067   \n",
              "3                              0.0           0.0    -0.029693     0.114113   \n",
              "4                              0.0           0.0    -0.072303     0.098135   \n",
              "...                            ...           ...          ...          ...   \n",
              "119860                         0.0           0.0    -0.023435    -0.042742   \n",
              "119861                         0.0           0.0    -0.056324    -0.040612   \n",
              "119862                         0.0           0.0    -0.037150     0.020106   \n",
              "119863                         0.0           0.0     0.005326     0.062716   \n",
              "119864                         0.0           0.0     0.036218    -0.002264   \n",
              "\n",
              "        Gyroscope Z  Acceleration X Change  Acceleration Y Change  \\\n",
              "0          0.006924               0.000000               0.000000   \n",
              "1          0.101730               0.000000               0.000000   \n",
              "2          0.123034               0.000000               0.000000   \n",
              "3          0.088947               0.000000               0.000000   \n",
              "4          0.113447               0.000000               0.000000   \n",
              "...             ...                    ...                    ...   \n",
              "119860     0.042876               0.172020               0.885110   \n",
              "119861    -0.006125               0.453840               0.153720   \n",
              "119862    -0.095605              -0.634095              -0.175680   \n",
              "119863    -0.018908              -0.293410              -0.297985   \n",
              "119864     0.090811              -0.414800               0.690825   \n",
              "\n",
              "        Acceleration Z Change  Roll (ยบ)  Pitch (ยบ)   Yaw (ยบ)  \\\n",
              "0                    0.000000 -19.75767  -11.26569  0.944726   \n",
              "1                    0.000000 -19.66945  -11.03860  0.851585   \n",
              "2                    0.000000 -19.68774  -10.73308  0.631760   \n",
              "3                    0.000000 -19.80198  -10.73826  0.417986   \n",
              "4                    0.000000 -19.72018  -10.74029  0.188188   \n",
              "...                       ...       ...        ...       ...   \n",
              "119860              -0.186050   0.00000    0.00000  0.000000   \n",
              "119861              -0.135421   0.00000    0.00000  0.000000   \n",
              "119862              -0.034160   0.00000    0.00000  0.000000   \n",
              "119863              -0.127490   0.00000    0.00000  0.000000   \n",
              "119864              -0.132370   0.00000    0.00000  0.000000   \n",
              "\n",
              "        Magnetometer X Change  Magnetometer Y Change  Magnetometer Z Change  \n",
              "0                    0.000000                0.00000                0.00000  \n",
              "1                    0.000000                0.00000                0.00000  \n",
              "2                    0.000000                0.00000                0.00000  \n",
              "3                    0.000000                0.00000                0.00000  \n",
              "4                    0.000000                0.00000                0.00000  \n",
              "...                       ...                    ...                    ...  \n",
              "119860              -1.199999               -0.88125                0.80625  \n",
              "119861              -1.331250               -0.95625                0.91875  \n",
              "119862              -0.843749               -1.01250                0.50625  \n",
              "119863              -0.450000               -0.76875                0.31875  \n",
              "119864              -0.618751               -0.48750                0.18750  \n",
              "\n",
              "[119865 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-600194ba-a94e-4376-911b-d58497c7cdf0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Latitude Change</th>\n",
              "      <th>Longitude Change</th>\n",
              "      <th>Altitude Change</th>\n",
              "      <th>Horizontal Accuracy Change</th>\n",
              "      <th>Speed Change</th>\n",
              "      <th>Gyroscope X</th>\n",
              "      <th>Gyroscope Y</th>\n",
              "      <th>Gyroscope Z</th>\n",
              "      <th>Acceleration X Change</th>\n",
              "      <th>Acceleration Y Change</th>\n",
              "      <th>Acceleration Z Change</th>\n",
              "      <th>Roll (ยบ)</th>\n",
              "      <th>Pitch (ยบ)</th>\n",
              "      <th>Yaw (ยบ)</th>\n",
              "      <th>Magnetometer X Change</th>\n",
              "      <th>Magnetometer Y Change</th>\n",
              "      <th>Magnetometer Z Change</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.063781</td>\n",
              "      <td>0.059786</td>\n",
              "      <td>0.006924</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.75767</td>\n",
              "      <td>-11.26569</td>\n",
              "      <td>0.944726</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.042476</td>\n",
              "      <td>0.115178</td>\n",
              "      <td>0.101730</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.66945</td>\n",
              "      <td>-11.03860</td>\n",
              "      <td>0.851585</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053395</td>\n",
              "      <td>0.148067</td>\n",
              "      <td>0.123034</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.68774</td>\n",
              "      <td>-10.73308</td>\n",
              "      <td>0.631760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.029693</td>\n",
              "      <td>0.114113</td>\n",
              "      <td>0.088947</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.80198</td>\n",
              "      <td>-10.73826</td>\n",
              "      <td>0.417986</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.072303</td>\n",
              "      <td>0.098135</td>\n",
              "      <td>0.113447</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.72018</td>\n",
              "      <td>-10.74029</td>\n",
              "      <td>0.188188</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119860</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.023435</td>\n",
              "      <td>-0.042742</td>\n",
              "      <td>0.042876</td>\n",
              "      <td>0.172020</td>\n",
              "      <td>0.885110</td>\n",
              "      <td>-0.186050</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.199999</td>\n",
              "      <td>-0.88125</td>\n",
              "      <td>0.80625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119861</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.056324</td>\n",
              "      <td>-0.040612</td>\n",
              "      <td>-0.006125</td>\n",
              "      <td>0.453840</td>\n",
              "      <td>0.153720</td>\n",
              "      <td>-0.135421</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.331250</td>\n",
              "      <td>-0.95625</td>\n",
              "      <td>0.91875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119862</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.037150</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>-0.095605</td>\n",
              "      <td>-0.634095</td>\n",
              "      <td>-0.175680</td>\n",
              "      <td>-0.034160</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.843749</td>\n",
              "      <td>-1.01250</td>\n",
              "      <td>0.50625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119863</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005326</td>\n",
              "      <td>0.062716</td>\n",
              "      <td>-0.018908</td>\n",
              "      <td>-0.293410</td>\n",
              "      <td>-0.297985</td>\n",
              "      <td>-0.127490</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.450000</td>\n",
              "      <td>-0.76875</td>\n",
              "      <td>0.31875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119864</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036218</td>\n",
              "      <td>-0.002264</td>\n",
              "      <td>0.090811</td>\n",
              "      <td>-0.414800</td>\n",
              "      <td>0.690825</td>\n",
              "      <td>-0.132370</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.618751</td>\n",
              "      <td>-0.48750</td>\n",
              "      <td>0.18750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>119865 rows ร 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-600194ba-a94e-4376-911b-d58497c7cdf0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-600194ba-a94e-4376-911b-d58497c7cdf0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-600194ba-a94e-4376-911b-d58497c7cdf0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0cb42328-97dc-4fbb-a672-a27226a8e7eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0cb42328-97dc-4fbb-a672-a27226a8e7eb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0cb42328-97dc-4fbb-a672-a27226a8e7eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_50af0aa1-c5d0-433d-be71-a51fd03dbd3b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_50af0aa1-c5d0-433d-be71-a51fd03dbd3b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2usI59lsUOE"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CER796QlYaBH",
        "outputId": "ac49cca1-76f1-468a-f046-78b3f15c8878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.5017 - loss: 2.0570 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6073 - loss: 0.8043 - learning_rate: 9.5000e-04\n",
            "Epoch 3/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.6224 - loss: 0.7622 - learning_rate: 9.0250e-04\n",
            "Epoch 4/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.6281 - loss: 0.7414 - learning_rate: 8.5737e-04\n",
            "Epoch 5/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.6345 - loss: 0.7293 - learning_rate: 8.1451e-04\n",
            "Epoch 6/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6422 - loss: 0.7160 - learning_rate: 7.7378e-04\n",
            "Epoch 7/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6456 - loss: 0.7095 - learning_rate: 7.3509e-04\n",
            "Epoch 8/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.6462 - loss: 0.7046 - learning_rate: 6.9834e-04\n",
            "Epoch 9/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.6503 - loss: 0.7008 - learning_rate: 6.6342e-04\n",
            "Epoch 10/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.6490 - loss: 0.6978 - learning_rate: 6.3025e-04\n",
            "Epoch 11/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - accuracy: 0.6492 - loss: 0.6959 - learning_rate: 5.9874e-04\n",
            "Epoch 12/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6515 - loss: 0.6917 - learning_rate: 5.6880e-04\n",
            "Epoch 13/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6514 - loss: 0.6891 - learning_rate: 5.4036e-04\n",
            "Epoch 14/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.6530 - loss: 0.6872 - learning_rate: 5.1334e-04\n",
            "Epoch 15/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.6537 - loss: 0.6839 - learning_rate: 4.8767e-04\n",
            "Epoch 16/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6544 - loss: 0.6822 - learning_rate: 4.6329e-04\n",
            "Epoch 17/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6577 - loss: 0.6777 - learning_rate: 4.4013e-04\n",
            "Epoch 18/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6556 - loss: 0.6777 - learning_rate: 4.1812e-04\n",
            "Epoch 19/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6557 - loss: 0.6744 - learning_rate: 3.9721e-04\n",
            "Epoch 20/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.6605 - loss: 0.6707 - learning_rate: 3.7735e-04\n",
            "Epoch 21/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6627 - loss: 0.6674 - learning_rate: 3.5849e-04\n",
            "Epoch 22/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6613 - loss: 0.6666 - learning_rate: 3.4056e-04\n",
            "Epoch 23/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6658 - loss: 0.6637 - learning_rate: 3.2353e-04\n",
            "Epoch 24/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.6612 - loss: 0.6660 - learning_rate: 3.0736e-04\n",
            "Epoch 25/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6644 - loss: 0.6639 - learning_rate: 2.9199e-04\n",
            "Epoch 26/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6659 - loss: 0.6615 - learning_rate: 2.7739e-04\n",
            "Epoch 27/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.6648 - loss: 0.6592 - learning_rate: 2.6352e-04\n",
            "Epoch 28/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6652 - loss: 0.6606 - learning_rate: 2.5034e-04\n",
            "Epoch 29/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6685 - loss: 0.6566 - learning_rate: 2.3783e-04\n",
            "Epoch 30/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6661 - loss: 0.6590 - learning_rate: 2.2594e-04\n",
            "Epoch 31/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.6682 - loss: 0.6559 - learning_rate: 2.1464e-04\n",
            "Epoch 32/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.6675 - loss: 0.6541 - learning_rate: 2.0391e-04\n",
            "Epoch 33/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6699 - loss: 0.6534 - learning_rate: 1.9371e-04\n",
            "Epoch 34/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6716 - loss: 0.6520 - learning_rate: 1.8403e-04\n",
            "Epoch 35/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6719 - loss: 0.6499 - learning_rate: 1.7482e-04\n",
            "Epoch 36/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6714 - loss: 0.6501 - learning_rate: 1.6608e-04\n",
            "Epoch 37/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.6760 - loss: 0.6475 - learning_rate: 1.5778e-04\n",
            "Epoch 38/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.6746 - loss: 0.6478 - learning_rate: 1.4989e-04\n",
            "Epoch 39/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.6756 - loss: 0.6444 - learning_rate: 1.4240e-04\n",
            "Epoch 40/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.6779 - loss: 0.6434 - learning_rate: 1.3528e-04\n",
            "Epoch 41/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6785 - loss: 0.6411 - learning_rate: 1.2851e-04\n",
            "Epoch 42/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6764 - loss: 0.6407 - learning_rate: 1.2209e-04\n",
            "Epoch 43/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6760 - loss: 0.6434 - learning_rate: 1.1598e-04\n",
            "Epoch 44/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.6777 - loss: 0.6396 - learning_rate: 1.1018e-04\n",
            "Epoch 45/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6834 - loss: 0.6362 - learning_rate: 1.0467e-04\n",
            "Epoch 46/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6775 - loss: 0.6384 - learning_rate: 9.9440e-05\n",
            "Epoch 47/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6801 - loss: 0.6374 - learning_rate: 9.4468e-05\n",
            "Epoch 48/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6830 - loss: 0.6369 - learning_rate: 8.9745e-05\n",
            "Epoch 49/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6848 - loss: 0.6305 - learning_rate: 8.5258e-05\n",
            "Epoch 50/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6809 - loss: 0.6371 - learning_rate: 8.0995e-05\n",
            "Epoch 51/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6838 - loss: 0.6340 - learning_rate: 7.6945e-05\n",
            "Epoch 52/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6830 - loss: 0.6365 - learning_rate: 7.3098e-05\n",
            "Epoch 53/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6824 - loss: 0.6351 - learning_rate: 6.9443e-05\n",
            "Epoch 54/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6843 - loss: 0.6300 - learning_rate: 6.5971e-05\n",
            "Epoch 55/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6841 - loss: 0.6303 - learning_rate: 6.2672e-05\n",
            "Epoch 56/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6849 - loss: 0.6283 - learning_rate: 5.9539e-05\n",
            "Epoch 57/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6852 - loss: 0.6290 - learning_rate: 5.6562e-05\n",
            "Epoch 58/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6863 - loss: 0.6273 - learning_rate: 5.3734e-05\n",
            "Epoch 59/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6881 - loss: 0.6290 - learning_rate: 5.1047e-05\n",
            "Epoch 60/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6874 - loss: 0.6274 - learning_rate: 4.8495e-05\n",
            "Epoch 61/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.6873 - loss: 0.6288 - learning_rate: 4.6070e-05\n",
            "Epoch 62/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6894 - loss: 0.6283 - learning_rate: 4.3766e-05\n",
            "Epoch 63/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.6880 - loss: 0.6274 - learning_rate: 4.1578e-05\n",
            "Epoch 64/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6876 - loss: 0.6281 - learning_rate: 3.9499e-05\n",
            "Epoch 65/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.6859 - loss: 0.6255 - learning_rate: 3.7524e-05\n",
            "Epoch 66/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.6884 - loss: 0.6262 - learning_rate: 3.5648e-05\n",
            "Epoch 67/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.6889 - loss: 0.6222 - learning_rate: 3.3866e-05\n",
            "Epoch 68/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.6895 - loss: 0.6241 - learning_rate: 3.2172e-05\n",
            "Epoch 69/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.6878 - loss: 0.6254 - learning_rate: 3.0564e-05\n",
            "Epoch 70/70\n",
            "\u001b[1m937/937\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6877 - loss: 0.6272 - learning_rate: 2.9035e-05\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "input_shape = X.shape[1]\n",
        "model = models.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=(input_shape,)),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "def lr_schedule(epoch):\n",
        "    return initial_learning_rate * 0.95 ** epoch\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, epochs=70, batch_size=128, callbacks=[lr_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "IVPrTPeg2DoU",
        "outputId": "e72f4ed5-c9ce-4e16-b859-d5594c4e9eb4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-16b63f01d90c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ],
      "source": [
        "model.evaluate(X_test, y_test, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "0JDv4rcu2Fbx",
        "outputId": "40e56c80-590c-4887-df48-a913222989cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1: Loss = 0.7448555827140808, Accuracy = 1.0\n",
            "Sample 2: Loss = 1.4129648208618164, Accuracy = 0.0\n",
            "Sample 3: Loss = 0.29115408658981323, Accuracy = 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 47534 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x791d3bf2e440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 4: Loss = 0.6059539914131165, Accuracy = 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 47535 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x791d3bf2e440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 5: Loss = 0.7140113711357117, Accuracy = 1.0\n",
            "Sample 6: Loss = 0.6458653211593628, Accuracy = 1.0\n",
            "Sample 7: Loss = 0.12740904092788696, Accuracy = 1.0\n",
            "Sample 8: Loss = 0.4114813506603241, Accuracy = 1.0\n",
            "Sample 9: Loss = 0.6383304595947266, Accuracy = 1.0\n",
            "Sample 10: Loss = 5.686121585313231e-05, Accuracy = 1.0\n",
            "Sample 11: Loss = 0.6639337539672852, Accuracy = 1.0\n",
            "Sample 12: Loss = 1.0066252946853638, Accuracy = 0.0\n",
            "Sample 13: Loss = 0.21373838186264038, Accuracy = 1.0\n",
            "Sample 14: Loss = 1.0043787956237793, Accuracy = 0.0\n",
            "Sample 15: Loss = 0.38047996163368225, Accuracy = 1.0\n",
            "Sample 16: Loss = 0.8480755686759949, Accuracy = 0.0\n",
            "Sample 17: Loss = 0.6328964829444885, Accuracy = 1.0\n",
            "Sample 18: Loss = 1.4013720750808716, Accuracy = 0.0\n",
            "Sample 19: Loss = 0.7383908033370972, Accuracy = 0.0\n",
            "Sample 20: Loss = 1.2535326480865479, Accuracy = 0.0\n",
            "Sample 21: Loss = 0.8411253690719604, Accuracy = 0.0\n",
            "Sample 22: Loss = 2.557854652404785, Accuracy = 0.0\n",
            "Sample 23: Loss = 0.9980297088623047, Accuracy = 0.0\n",
            "Sample 24: Loss = 0.12707661092281342, Accuracy = 1.0\n",
            "Sample 25: Loss = 0.3294198513031006, Accuracy = 1.0\n",
            "Sample 26: Loss = 0.6077448129653931, Accuracy = 1.0\n",
            "Sample 27: Loss = 0.5527652502059937, Accuracy = 1.0\n",
            "Sample 28: Loss = 0.795003354549408, Accuracy = 1.0\n",
            "Sample 29: Loss = 0.1932247281074524, Accuracy = 1.0\n",
            "Sample 30: Loss = 0.017383594065904617, Accuracy = 1.0\n",
            "Sample 31: Loss = 2.274355173110962, Accuracy = 0.0\n",
            "Sample 32: Loss = 0.013539978303015232, Accuracy = 1.0\n",
            "Sample 33: Loss = 1.2395901679992676, Accuracy = 0.0\n",
            "Sample 34: Loss = 0.5607357025146484, Accuracy = 1.0\n",
            "Sample 35: Loss = 0.6173766851425171, Accuracy = 1.0\n",
            "Sample 36: Loss = 0.5825902223587036, Accuracy = 1.0\n",
            "Sample 37: Loss = 0.4230877757072449, Accuracy = 1.0\n",
            "Sample 38: Loss = 0.32364019751548767, Accuracy = 1.0\n",
            "Sample 39: Loss = 0.5894688367843628, Accuracy = 1.0\n",
            "Sample 40: Loss = 0.21556456387043, Accuracy = 1.0\n",
            "Sample 41: Loss = 0.08137673884630203, Accuracy = 1.0\n",
            "Sample 42: Loss = 1.1223738193511963, Accuracy = 0.0\n",
            "Sample 43: Loss = 0.46872416138648987, Accuracy = 1.0\n",
            "Sample 44: Loss = 1.39522123336792, Accuracy = 0.0\n",
            "Sample 45: Loss = 2.7000348567962646, Accuracy = 0.0\n",
            "Sample 46: Loss = 0.618724524974823, Accuracy = 1.0\n",
            "Sample 47: Loss = 0.7994933724403381, Accuracy = 1.0\n",
            "Sample 48: Loss = 0.5664631128311157, Accuracy = 1.0\n",
            "Sample 49: Loss = 0.24294662475585938, Accuracy = 1.0\n",
            "Sample 50: Loss = 0.5256276726722717, Accuracy = 1.0\n",
            "Sample 51: Loss = 0.7505211234092712, Accuracy = 1.0\n",
            "Sample 52: Loss = 0.06271177530288696, Accuracy = 1.0\n",
            "Sample 53: Loss = 0.22482994198799133, Accuracy = 1.0\n",
            "Sample 54: Loss = 0.7609943747520447, Accuracy = 0.0\n",
            "Sample 55: Loss = 0.6449439525604248, Accuracy = 1.0\n",
            "Sample 56: Loss = 0.36315473914146423, Accuracy = 1.0\n",
            "Sample 57: Loss = 0.8708604574203491, Accuracy = 0.0\n",
            "Sample 58: Loss = 1.9262341260910034, Accuracy = 0.0\n",
            "Sample 59: Loss = 0.7190846800804138, Accuracy = 1.0\n",
            "Sample 60: Loss = 0.4249553978443146, Accuracy = 1.0\n",
            "Sample 61: Loss = 0.8329966068267822, Accuracy = 0.0\n",
            "Sample 62: Loss = 1.572610855102539, Accuracy = 0.0\n",
            "Sample 63: Loss = 0.26820817589759827, Accuracy = 1.0\n",
            "Sample 64: Loss = 0.22324225306510925, Accuracy = 1.0\n",
            "Sample 65: Loss = 0.7338001728057861, Accuracy = 0.0\n",
            "Sample 66: Loss = 0.7224091291427612, Accuracy = 0.0\n",
            "Sample 67: Loss = 0.6533717513084412, Accuracy = 1.0\n",
            "Sample 68: Loss = 0.17875884473323822, Accuracy = 1.0\n",
            "Sample 69: Loss = 0.7089752554893494, Accuracy = 1.0\n",
            "Sample 70: Loss = 0.6808598637580872, Accuracy = 1.0\n",
            "Sample 71: Loss = 1.0594316720962524, Accuracy = 0.0\n",
            "Sample 72: Loss = 0.9380896091461182, Accuracy = 0.0\n",
            "Sample 73: Loss = 0.7804847359657288, Accuracy = 1.0\n",
            "Sample 74: Loss = 0.6166157722473145, Accuracy = 1.0\n",
            "Sample 75: Loss = 0.8598196506500244, Accuracy = 0.0\n",
            "Sample 76: Loss = 0.05843719467520714, Accuracy = 1.0\n",
            "Sample 77: Loss = 0.784937858581543, Accuracy = 0.0\n",
            "Sample 78: Loss = 0.14855818450450897, Accuracy = 1.0\n",
            "Sample 79: Loss = 0.00018666432879399508, Accuracy = 1.0\n",
            "Sample 80: Loss = 0.32078197598457336, Accuracy = 1.0\n",
            "Sample 81: Loss = 0.5594642758369446, Accuracy = 1.0\n",
            "Sample 82: Loss = 0.010414304211735725, Accuracy = 1.0\n",
            "Sample 83: Loss = 0.36491018533706665, Accuracy = 1.0\n",
            "Sample 84: Loss = 0.582412838935852, Accuracy = 1.0\n",
            "Sample 85: Loss = 0.5174466967582703, Accuracy = 1.0\n",
            "Sample 86: Loss = 1.096414566040039, Accuracy = 0.0\n",
            "Sample 87: Loss = 1.3505585193634033, Accuracy = 0.0\n",
            "Sample 88: Loss = 0.6877134442329407, Accuracy = 1.0\n",
            "Sample 89: Loss = 0.39445027709007263, Accuracy = 1.0\n",
            "Sample 90: Loss = 0.49297210574150085, Accuracy = 1.0\n",
            "Sample 91: Loss = 0.02971118874847889, Accuracy = 1.0\n",
            "Sample 92: Loss = 0.28357967734336853, Accuracy = 1.0\n",
            "Sample 93: Loss = 0.39131513237953186, Accuracy = 1.0\n",
            "Sample 94: Loss = 0.5097106695175171, Accuracy = 1.0\n",
            "Sample 95: Loss = 1.6567531824111938, Accuracy = 0.0\n",
            "Sample 96: Loss = 0.4933425188064575, Accuracy = 1.0\n",
            "Sample 97: Loss = 0.43287140130996704, Accuracy = 1.0\n",
            "Sample 98: Loss = 0.587451696395874, Accuracy = 1.0\n",
            "Sample 99: Loss = 0.5321719646453857, Accuracy = 1.0\n",
            "Sample 100: Loss = 0.6420164108276367, Accuracy = 1.0\n",
            "Sample 101: Loss = 2.1060824394226074, Accuracy = 0.0\n",
            "Sample 102: Loss = 0.17480948567390442, Accuracy = 1.0\n",
            "Sample 103: Loss = 0.6291284561157227, Accuracy = 1.0\n",
            "Sample 104: Loss = 2.6416144371032715, Accuracy = 0.0\n",
            "Sample 105: Loss = 0.36825793981552124, Accuracy = 1.0\n",
            "Sample 106: Loss = 0.7176154255867004, Accuracy = 1.0\n",
            "Sample 107: Loss = 0.9724041819572449, Accuracy = 0.0\n",
            "Sample 108: Loss = 0.39913082122802734, Accuracy = 1.0\n",
            "Sample 109: Loss = 1.044011116027832, Accuracy = 0.0\n",
            "Sample 110: Loss = 1.0858732461929321, Accuracy = 0.0\n",
            "Sample 111: Loss = 0.823704719543457, Accuracy = 0.0\n",
            "Sample 112: Loss = 2.7408175468444824, Accuracy = 0.0\n",
            "Sample 113: Loss = 0.5563222169876099, Accuracy = 1.0\n",
            "Sample 114: Loss = 0.2826634645462036, Accuracy = 1.0\n",
            "Sample 115: Loss = 0.5492862462997437, Accuracy = 1.0\n",
            "Sample 116: Loss = 0.3355741798877716, Accuracy = 1.0\n",
            "Sample 117: Loss = 0.5329057574272156, Accuracy = 1.0\n",
            "Sample 118: Loss = 0.5095465779304504, Accuracy = 1.0\n",
            "Sample 119: Loss = 0.4785560667514801, Accuracy = 1.0\n",
            "Sample 120: Loss = 0.5643274784088135, Accuracy = 1.0\n",
            "Sample 121: Loss = 0.3172760307788849, Accuracy = 1.0\n",
            "Sample 122: Loss = 1.2581493854522705, Accuracy = 0.0\n",
            "Sample 123: Loss = 0.765692949295044, Accuracy = 0.0\n",
            "Sample 124: Loss = 0.3836466073989868, Accuracy = 1.0\n",
            "Sample 125: Loss = 0.4309474229812622, Accuracy = 1.0\n",
            "Sample 126: Loss = 0.2968393862247467, Accuracy = 1.0\n",
            "Sample 127: Loss = 1.7325927019119263, Accuracy = 0.0\n",
            "Sample 128: Loss = 1.1920928244535389e-07, Accuracy = 1.0\n",
            "Sample 129: Loss = 1.1480062007904053, Accuracy = 0.0\n",
            "Sample 130: Loss = 0.6030949354171753, Accuracy = 1.0\n",
            "Sample 131: Loss = 1.004007339477539, Accuracy = 0.0\n",
            "Sample 132: Loss = 0.6711516976356506, Accuracy = 1.0\n",
            "Sample 133: Loss = 0.804268479347229, Accuracy = 0.0\n",
            "Sample 134: Loss = 0.5919038653373718, Accuracy = 1.0\n",
            "Sample 135: Loss = 0.466191828250885, Accuracy = 1.0\n",
            "Sample 136: Loss = 0.5229811668395996, Accuracy = 1.0\n",
            "Sample 137: Loss = 0.5971608757972717, Accuracy = 1.0\n",
            "Sample 138: Loss = 0.0485028401017189, Accuracy = 1.0\n",
            "Sample 139: Loss = 0.44380828738212585, Accuracy = 1.0\n",
            "Sample 140: Loss = 2.9347939491271973, Accuracy = 0.0\n",
            "Sample 141: Loss = 0.03234577178955078, Accuracy = 1.0\n",
            "Sample 142: Loss = 0.4624665379524231, Accuracy = 1.0\n",
            "Sample 143: Loss = 2.516592025756836, Accuracy = 0.0\n",
            "Sample 144: Loss = 0.6096863150596619, Accuracy = 1.0\n",
            "Sample 145: Loss = 0.2931835353374481, Accuracy = 1.0\n",
            "Sample 146: Loss = 0.49325108528137207, Accuracy = 1.0\n",
            "Sample 147: Loss = 0.1380101889371872, Accuracy = 1.0\n",
            "Sample 148: Loss = 1.0708825588226318, Accuracy = 0.0\n",
            "Sample 149: Loss = 0.4412282407283783, Accuracy = 1.0\n",
            "Sample 150: Loss = 0.7550821304321289, Accuracy = 0.0\n",
            "Sample 151: Loss = 1.0633567571640015, Accuracy = 0.0\n",
            "Sample 152: Loss = 0.5930705666542053, Accuracy = 1.0\n",
            "Sample 153: Loss = 0.35287007689476013, Accuracy = 1.0\n",
            "Sample 154: Loss = 0.6167449951171875, Accuracy = 1.0\n",
            "Sample 155: Loss = 0.9609085917472839, Accuracy = 0.0\n",
            "Sample 156: Loss = 0.33591228723526, Accuracy = 1.0\n",
            "Sample 157: Loss = 0.568354070186615, Accuracy = 1.0\n",
            "Sample 158: Loss = 0.22817134857177734, Accuracy = 1.0\n",
            "Sample 159: Loss = 0.1371983289718628, Accuracy = 1.0\n",
            "Sample 160: Loss = 0.9138731360435486, Accuracy = 0.0\n",
            "Sample 161: Loss = 2.6586503982543945, Accuracy = 0.0\n",
            "Sample 162: Loss = 0.7411130666732788, Accuracy = 1.0\n",
            "Sample 163: Loss = 0.45118337869644165, Accuracy = 1.0\n",
            "Sample 164: Loss = 0.7940318584442139, Accuracy = 1.0\n",
            "Sample 165: Loss = 0.4363216161727905, Accuracy = 1.0\n",
            "Sample 166: Loss = 0.3941391110420227, Accuracy = 1.0\n",
            "Sample 167: Loss = 1.1920928244535389e-07, Accuracy = 1.0\n",
            "Sample 168: Loss = 0.5382615923881531, Accuracy = 1.0\n",
            "Sample 169: Loss = 2.3841855067985307e-07, Accuracy = 1.0\n",
            "Sample 170: Loss = 0.450763076543808, Accuracy = 1.0\n",
            "Sample 171: Loss = 0.4007282853126526, Accuracy = 1.0\n",
            "Sample 172: Loss = 0.5190750956535339, Accuracy = 1.0\n",
            "Sample 173: Loss = 0.5042609572410583, Accuracy = 1.0\n",
            "Sample 174: Loss = 0.6467952728271484, Accuracy = 1.0\n",
            "Sample 175: Loss = 0.0644424557685852, Accuracy = 1.0\n",
            "Sample 176: Loss = 1.465630054473877, Accuracy = 0.0\n",
            "Sample 177: Loss = 0.32899022102355957, Accuracy = 1.0\n",
            "Sample 178: Loss = 0.8114230036735535, Accuracy = 0.0\n",
            "Sample 179: Loss = 0.44190648198127747, Accuracy = 1.0\n",
            "Sample 180: Loss = 0.827337920665741, Accuracy = 0.0\n",
            "Sample 181: Loss = 0.7437503337860107, Accuracy = 1.0\n",
            "Sample 182: Loss = 0.2005796730518341, Accuracy = 1.0\n",
            "Sample 183: Loss = 1.6936943531036377, Accuracy = 0.0\n",
            "Sample 184: Loss = 0.2581120729446411, Accuracy = 1.0\n",
            "Sample 185: Loss = 0.7532603740692139, Accuracy = 0.0\n",
            "Sample 186: Loss = 0.5965555310249329, Accuracy = 1.0\n",
            "Sample 187: Loss = 0.41155630350112915, Accuracy = 1.0\n",
            "Sample 188: Loss = 0.03912542015314102, Accuracy = 1.0\n",
            "Sample 189: Loss = 0.5204002261161804, Accuracy = 1.0\n",
            "Sample 190: Loss = 0.3973234295845032, Accuracy = 1.0\n",
            "Sample 191: Loss = 8.892617915989831e-05, Accuracy = 1.0\n",
            "Sample 192: Loss = 0.4251565933227539, Accuracy = 1.0\n",
            "Sample 193: Loss = 0.3689730763435364, Accuracy = 1.0\n",
            "Sample 194: Loss = 0.47756215929985046, Accuracy = 1.0\n",
            "Sample 195: Loss = 1.371002435684204, Accuracy = 0.0\n",
            "Sample 196: Loss = 0.4594291150569916, Accuracy = 1.0\n",
            "Sample 197: Loss = 0.22778953611850739, Accuracy = 1.0\n",
            "Sample 198: Loss = 0.38129690289497375, Accuracy = 1.0\n",
            "Sample 199: Loss = 0.8208038806915283, Accuracy = 0.0\n",
            "Sample 200: Loss = 3.0725018978118896, Accuracy = 0.0\n",
            "Sample 201: Loss = 0.3932127356529236, Accuracy = 1.0\n",
            "Sample 202: Loss = 0.31232044100761414, Accuracy = 1.0\n",
            "Sample 203: Loss = 0.23134025931358337, Accuracy = 1.0\n",
            "Sample 204: Loss = 0.4878336787223816, Accuracy = 1.0\n",
            "Sample 205: Loss = 3.1998391151428223, Accuracy = 0.0\n",
            "Sample 206: Loss = 0.8460376858711243, Accuracy = 0.0\n",
            "Sample 207: Loss = 0.1746828705072403, Accuracy = 1.0\n",
            "Sample 208: Loss = 0.508442223072052, Accuracy = 1.0\n",
            "Sample 209: Loss = 0.9214012622833252, Accuracy = 0.0\n",
            "Sample 210: Loss = 0.9083466529846191, Accuracy = 0.0\n",
            "Sample 211: Loss = 1.034318447113037, Accuracy = 0.0\n",
            "Sample 212: Loss = 0.17101840674877167, Accuracy = 1.0\n",
            "Sample 213: Loss = 0.8649226427078247, Accuracy = 0.0\n",
            "Sample 214: Loss = 0.8356053829193115, Accuracy = 0.0\n",
            "Sample 215: Loss = 0.6877643465995789, Accuracy = 1.0\n",
            "Sample 216: Loss = 0.7391160130500793, Accuracy = 1.0\n",
            "Sample 217: Loss = 4.768370445162873e-07, Accuracy = 1.0\n",
            "Sample 218: Loss = 0.954231321811676, Accuracy = 0.0\n",
            "Sample 219: Loss = 0.6202226877212524, Accuracy = 1.0\n",
            "Sample 220: Loss = 0.48005491495132446, Accuracy = 1.0\n",
            "Sample 221: Loss = 0.9901593923568726, Accuracy = 0.0\n",
            "Sample 222: Loss = 0.06390548497438431, Accuracy = 1.0\n",
            "Sample 223: Loss = 0.7421941161155701, Accuracy = 0.0\n",
            "Sample 224: Loss = 1.121010184288025, Accuracy = 0.0\n",
            "Sample 225: Loss = 1.1857807636260986, Accuracy = 0.0\n",
            "Sample 226: Loss = 0.7942451238632202, Accuracy = 0.0\n",
            "Sample 227: Loss = 0.46895936131477356, Accuracy = 1.0\n",
            "Sample 228: Loss = 0.5092524886131287, Accuracy = 1.0\n",
            "Sample 229: Loss = 0.3472898006439209, Accuracy = 1.0\n",
            "Sample 230: Loss = 0.5245392322540283, Accuracy = 1.0\n",
            "Sample 231: Loss = 0.40283331274986267, Accuracy = 1.0\n",
            "Sample 232: Loss = 0.13735462725162506, Accuracy = 1.0\n",
            "Sample 233: Loss = 0.738216757774353, Accuracy = 0.0\n",
            "Sample 234: Loss = 1.0640513896942139, Accuracy = 0.0\n",
            "Sample 235: Loss = 0.9031774401664734, Accuracy = 1.0\n",
            "Sample 236: Loss = 1.9090604782104492, Accuracy = 0.0\n",
            "Sample 237: Loss = 0.8297531604766846, Accuracy = 0.0\n",
            "Sample 238: Loss = 0.6804914474487305, Accuracy = 1.0\n",
            "Sample 239: Loss = 1.9073468138230965e-06, Accuracy = 1.0\n",
            "Sample 240: Loss = 0.8968257904052734, Accuracy = 0.0\n",
            "Sample 241: Loss = 0.3502150774002075, Accuracy = 1.0\n",
            "Sample 242: Loss = 0.7397249341011047, Accuracy = 0.0\n",
            "Sample 243: Loss = 0.7107354402542114, Accuracy = 1.0\n",
            "Sample 244: Loss = 0.8424198627471924, Accuracy = 0.0\n",
            "Sample 245: Loss = 0.4681393504142761, Accuracy = 1.0\n",
            "Sample 246: Loss = 0.2447831928730011, Accuracy = 1.0\n",
            "Sample 247: Loss = 0.45597609877586365, Accuracy = 1.0\n",
            "Sample 248: Loss = 0.643036425113678, Accuracy = 1.0\n",
            "Sample 249: Loss = 1.4838125705718994, Accuracy = 0.0\n",
            "Sample 250: Loss = 0.7267940640449524, Accuracy = 0.0\n",
            "Sample 251: Loss = 0.5156877040863037, Accuracy = 1.0\n",
            "Sample 252: Loss = 0.698732852935791, Accuracy = 1.0\n",
            "Sample 253: Loss = 1.0339539051055908, Accuracy = 0.0\n",
            "Sample 254: Loss = 0.3881625235080719, Accuracy = 1.0\n",
            "Sample 255: Loss = 0.03177384287118912, Accuracy = 1.0\n",
            "Sample 256: Loss = 0.25444847345352173, Accuracy = 1.0\n",
            "Sample 257: Loss = 0.5574459433555603, Accuracy = 1.0\n",
            "Sample 258: Loss = 0.35486921668052673, Accuracy = 1.0\n",
            "Sample 259: Loss = 0.4484824240207672, Accuracy = 1.0\n",
            "Sample 260: Loss = 0.40647009015083313, Accuracy = 1.0\n",
            "Sample 261: Loss = 0.9122678637504578, Accuracy = 0.0\n",
            "Sample 262: Loss = 0.8340078592300415, Accuracy = 0.0\n",
            "Sample 263: Loss = 0.41822269558906555, Accuracy = 1.0\n",
            "Sample 264: Loss = 0.9962821006774902, Accuracy = 0.0\n",
            "Sample 265: Loss = 1.6433804035186768, Accuracy = 0.0\n",
            "Sample 266: Loss = 1.4645490646362305, Accuracy = 0.0\n",
            "Sample 267: Loss = 0.9315673112869263, Accuracy = 0.0\n",
            "Sample 268: Loss = 0.6445002555847168, Accuracy = 1.0\n",
            "Sample 269: Loss = 1.443028450012207, Accuracy = 0.0\n",
            "Sample 270: Loss = 0.3734061121940613, Accuracy = 1.0\n",
            "Sample 271: Loss = 0.26668301224708557, Accuracy = 1.0\n",
            "Sample 272: Loss = 0.2852880358695984, Accuracy = 1.0\n",
            "Sample 273: Loss = 0.4696047306060791, Accuracy = 1.0\n",
            "Sample 274: Loss = 0.49299854040145874, Accuracy = 1.0\n",
            "Sample 275: Loss = 1.1189703941345215, Accuracy = 0.0\n",
            "Sample 276: Loss = 0.4948437213897705, Accuracy = 1.0\n",
            "Sample 277: Loss = 1.637969970703125, Accuracy = 0.0\n",
            "Sample 278: Loss = 0.5024535655975342, Accuracy = 1.0\n",
            "Sample 279: Loss = 0.5021282434463501, Accuracy = 1.0\n",
            "Sample 280: Loss = 0.8392350673675537, Accuracy = 0.0\n",
            "Sample 281: Loss = 0.4549047350883484, Accuracy = 1.0\n",
            "Sample 282: Loss = 0.8292255401611328, Accuracy = 0.0\n",
            "Sample 283: Loss = 0.5889717936515808, Accuracy = 1.0\n",
            "Sample 284: Loss = 0.35848337411880493, Accuracy = 1.0\n",
            "Sample 285: Loss = 0.9663029909133911, Accuracy = 0.0\n",
            "Sample 286: Loss = 0.992499589920044, Accuracy = 0.0\n",
            "Sample 287: Loss = 0.2832285761833191, Accuracy = 1.0\n",
            "Sample 288: Loss = 1.3494564294815063, Accuracy = 0.0\n",
            "Sample 289: Loss = 0.40454766154289246, Accuracy = 1.0\n",
            "Sample 290: Loss = 0.49673357605934143, Accuracy = 1.0\n",
            "Sample 291: Loss = 0.40627509355545044, Accuracy = 1.0\n",
            "Sample 292: Loss = 0.8010950684547424, Accuracy = 1.0\n",
            "Sample 293: Loss = 0.3928918242454529, Accuracy = 1.0\n",
            "Sample 294: Loss = 0.7530597448348999, Accuracy = 1.0\n",
            "Sample 295: Loss = 1.3284679651260376, Accuracy = 0.0\n",
            "Sample 296: Loss = 0.4768100082874298, Accuracy = 1.0\n",
            "Sample 297: Loss = 0.3786473274230957, Accuracy = 1.0\n",
            "Sample 298: Loss = 0.6639218926429749, Accuracy = 1.0\n",
            "Sample 299: Loss = 0.41915178298950195, Accuracy = 1.0\n",
            "Sample 300: Loss = 0.5240299701690674, Accuracy = 1.0\n",
            "Sample 301: Loss = 0.24626438319683075, Accuracy = 1.0\n",
            "Sample 302: Loss = 0.9419974088668823, Accuracy = 0.0\n",
            "Sample 303: Loss = 0.37928786873817444, Accuracy = 1.0\n",
            "Sample 304: Loss = 0.05397519841790199, Accuracy = 1.0\n",
            "Sample 305: Loss = 0.7356674671173096, Accuracy = 1.0\n",
            "Sample 306: Loss = 0.45870593190193176, Accuracy = 1.0\n",
            "Sample 307: Loss = 0.6809358596801758, Accuracy = 1.0\n",
            "Sample 308: Loss = 0.0024991966784000397, Accuracy = 1.0\n",
            "Sample 309: Loss = 0.8950930833816528, Accuracy = 0.0\n",
            "Sample 310: Loss = 0.5055138468742371, Accuracy = 1.0\n",
            "Sample 311: Loss = 0.40069109201431274, Accuracy = 1.0\n",
            "Sample 312: Loss = 0.42522844672203064, Accuracy = 1.0\n",
            "Sample 313: Loss = 0.4641515612602234, Accuracy = 1.0\n",
            "Sample 314: Loss = 0.6527008414268494, Accuracy = 1.0\n",
            "Sample 315: Loss = 0.4816933274269104, Accuracy = 1.0\n",
            "Sample 316: Loss = 0.7621304988861084, Accuracy = 0.0\n",
            "Sample 317: Loss = 1.0209507942199707, Accuracy = 0.0\n",
            "Sample 318: Loss = 0.9902111291885376, Accuracy = 0.0\n",
            "Sample 319: Loss = 0.5892589688301086, Accuracy = 1.0\n",
            "Sample 320: Loss = 0.4493336081504822, Accuracy = 1.0\n",
            "Sample 321: Loss = 0.47161322832107544, Accuracy = 1.0\n",
            "Sample 322: Loss = 0.570692241191864, Accuracy = 1.0\n",
            "Sample 323: Loss = 0.4589662253856659, Accuracy = 1.0\n",
            "Sample 324: Loss = 1.0694937705993652, Accuracy = 0.0\n",
            "Sample 325: Loss = 0.7843393087387085, Accuracy = 0.0\n",
            "Sample 326: Loss = 0.005702892318367958, Accuracy = 1.0\n",
            "Sample 327: Loss = 0.4056229293346405, Accuracy = 1.0\n",
            "Sample 328: Loss = 0.6099469065666199, Accuracy = 1.0\n",
            "Sample 329: Loss = 0.14399173855781555, Accuracy = 1.0\n",
            "Sample 330: Loss = 0.20305567979812622, Accuracy = 1.0\n",
            "Sample 331: Loss = 1.0813641548156738, Accuracy = 0.0\n",
            "Sample 332: Loss = 0.6710898280143738, Accuracy = 1.0\n",
            "Sample 333: Loss = 0.017191559076309204, Accuracy = 1.0\n",
            "Sample 334: Loss = 0.10367170721292496, Accuracy = 1.0\n",
            "Sample 335: Loss = 0.03318287432193756, Accuracy = 1.0\n",
            "Sample 336: Loss = 0.00019298121333122253, Accuracy = 1.0\n",
            "Sample 337: Loss = 1.3203468322753906, Accuracy = 0.0\n",
            "Sample 338: Loss = 0.5045303702354431, Accuracy = 1.0\n",
            "Sample 339: Loss = 0.7501052021980286, Accuracy = 1.0\n",
            "Sample 340: Loss = 1.549692153930664, Accuracy = 0.0\n",
            "Sample 341: Loss = 0.49028557538986206, Accuracy = 1.0\n",
            "Sample 342: Loss = 0.175416961312294, Accuracy = 1.0\n",
            "Sample 343: Loss = 0.9011046290397644, Accuracy = 0.0\n",
            "Sample 344: Loss = 0.18087662756443024, Accuracy = 1.0\n",
            "Sample 345: Loss = 0.6218386292457581, Accuracy = 1.0\n",
            "Sample 346: Loss = 0.7819105982780457, Accuracy = 1.0\n",
            "Sample 347: Loss = 0.2133805900812149, Accuracy = 1.0\n",
            "Sample 348: Loss = 0.6845943927764893, Accuracy = 1.0\n",
            "Sample 349: Loss = 1.2212958335876465, Accuracy = 0.0\n",
            "Sample 350: Loss = 0.9865460991859436, Accuracy = 0.0\n",
            "Sample 351: Loss = 1.2360409498214722, Accuracy = 0.0\n",
            "Sample 352: Loss = 1.1192762851715088, Accuracy = 0.0\n",
            "Sample 353: Loss = 0.24634099006652832, Accuracy = 1.0\n",
            "Sample 354: Loss = 0.5726144909858704, Accuracy = 1.0\n",
            "Sample 355: Loss = 0.20128828287124634, Accuracy = 1.0\n",
            "Sample 356: Loss = 0.27904096245765686, Accuracy = 1.0\n",
            "Sample 357: Loss = 0.9161756038665771, Accuracy = 0.0\n",
            "Sample 358: Loss = 0.45516782999038696, Accuracy = 1.0\n",
            "Sample 359: Loss = 0.6803240776062012, Accuracy = 1.0\n",
            "Sample 360: Loss = 0.6085863709449768, Accuracy = 1.0\n",
            "Sample 361: Loss = 1.5594971179962158, Accuracy = 0.0\n",
            "Sample 362: Loss = 3.119868755340576, Accuracy = 0.0\n",
            "Sample 363: Loss = 0.3909872770309448, Accuracy = 1.0\n",
            "Sample 364: Loss = 0.7826932668685913, Accuracy = 0.0\n",
            "Sample 365: Loss = 0.46702873706817627, Accuracy = 1.0\n",
            "Sample 366: Loss = 0.6041596531867981, Accuracy = 1.0\n",
            "Sample 367: Loss = 0.5647686123847961, Accuracy = 1.0\n",
            "Sample 368: Loss = 0.683120846748352, Accuracy = 1.0\n",
            "Sample 369: Loss = 0.1460254192352295, Accuracy = 1.0\n",
            "Sample 370: Loss = 0.6729068756103516, Accuracy = 1.0\n",
            "Sample 371: Loss = 0.38162657618522644, Accuracy = 1.0\n",
            "Sample 372: Loss = 0.2677716016769409, Accuracy = 1.0\n",
            "Sample 373: Loss = 0.3059839904308319, Accuracy = 1.0\n",
            "Sample 374: Loss = 0.39250409603118896, Accuracy = 1.0\n",
            "Sample 375: Loss = 0.36016520857810974, Accuracy = 1.0\n",
            "Sample 376: Loss = 0.3285243809223175, Accuracy = 1.0\n",
            "Sample 377: Loss = 0.5114825367927551, Accuracy = 1.0\n",
            "Sample 378: Loss = 0.019029201939702034, Accuracy = 1.0\n",
            "Sample 379: Loss = 0.1850762963294983, Accuracy = 1.0\n",
            "Sample 380: Loss = 0.5855656266212463, Accuracy = 1.0\n",
            "Sample 381: Loss = 0.23947325348854065, Accuracy = 1.0\n",
            "Sample 382: Loss = 0.3032996952533722, Accuracy = 1.0\n",
            "Sample 383: Loss = 0.42235681414604187, Accuracy = 1.0\n",
            "Sample 384: Loss = 1.597996711730957, Accuracy = 0.0\n",
            "Sample 385: Loss = 0.3924369513988495, Accuracy = 1.0\n",
            "Sample 386: Loss = 0.2572844922542572, Accuracy = 1.0\n",
            "Sample 387: Loss = 0.31913208961486816, Accuracy = 1.0\n",
            "Sample 388: Loss = 1.4448821544647217, Accuracy = 0.0\n",
            "Sample 389: Loss = 0.42435672879219055, Accuracy = 1.0\n",
            "Sample 390: Loss = 0.9870045781135559, Accuracy = 0.0\n",
            "Sample 391: Loss = 0.0033950330689549446, Accuracy = 1.0\n",
            "Sample 392: Loss = 0.7370996475219727, Accuracy = 0.0\n",
            "Sample 393: Loss = 0.22369566559791565, Accuracy = 1.0\n",
            "Sample 394: Loss = 0.3692229390144348, Accuracy = 1.0\n",
            "Sample 395: Loss = 0.3400316834449768, Accuracy = 1.0\n",
            "Sample 396: Loss = 0.40866756439208984, Accuracy = 1.0\n",
            "Sample 397: Loss = 0.864637553691864, Accuracy = 0.0\n",
            "Sample 398: Loss = 0.6318655610084534, Accuracy = 1.0\n",
            "Sample 399: Loss = 0.29012832045555115, Accuracy = 1.0\n",
            "Sample 400: Loss = 0.7092059254646301, Accuracy = 0.0\n",
            "Sample 401: Loss = 1.3139355182647705, Accuracy = 0.0\n",
            "Sample 402: Loss = 2.067197322845459, Accuracy = 0.0\n",
            "Sample 403: Loss = 0.8621166944503784, Accuracy = 0.0\n",
            "Sample 404: Loss = 0.0024207117967307568, Accuracy = 1.0\n",
            "Sample 405: Loss = 0.19934216141700745, Accuracy = 1.0\n",
            "Sample 406: Loss = 0.5293252468109131, Accuracy = 1.0\n",
            "Sample 407: Loss = 0.8185696601867676, Accuracy = 0.0\n",
            "Sample 408: Loss = 2.5239927768707275, Accuracy = 0.0\n",
            "Sample 409: Loss = 0.3728012144565582, Accuracy = 1.0\n",
            "Sample 410: Loss = 0.634505033493042, Accuracy = 1.0\n",
            "Sample 411: Loss = 0.4689660668373108, Accuracy = 1.0\n",
            "Sample 412: Loss = 1.1379239559173584, Accuracy = 0.0\n",
            "Sample 413: Loss = 0.4446752369403839, Accuracy = 1.0\n",
            "Sample 414: Loss = 0.4001772403717041, Accuracy = 1.0\n",
            "Sample 415: Loss = 0.3651626408100128, Accuracy = 1.0\n",
            "Sample 416: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 417: Loss = 0.7938498258590698, Accuracy = 0.0\n",
            "Sample 418: Loss = 0.9585875272750854, Accuracy = 0.0\n",
            "Sample 419: Loss = 0.4874367117881775, Accuracy = 1.0\n",
            "Sample 420: Loss = 0.0011922164121642709, Accuracy = 1.0\n",
            "Sample 421: Loss = 0.00032085992279462516, Accuracy = 1.0\n",
            "Sample 422: Loss = 1.6148077249526978, Accuracy = 0.0\n",
            "Sample 423: Loss = 0.3769107162952423, Accuracy = 1.0\n",
            "Sample 424: Loss = 0.0760335773229599, Accuracy = 1.0\n",
            "Sample 425: Loss = 1.3316148519515991, Accuracy = 0.0\n",
            "Sample 426: Loss = 0.7094120383262634, Accuracy = 1.0\n",
            "Sample 427: Loss = 0.5738821625709534, Accuracy = 1.0\n",
            "Sample 428: Loss = 0.38186901807785034, Accuracy = 1.0\n",
            "Sample 429: Loss = 0.5927612781524658, Accuracy = 1.0\n",
            "Sample 430: Loss = 0.21088379621505737, Accuracy = 1.0\n",
            "Sample 431: Loss = 0.6296481490135193, Accuracy = 1.0\n",
            "Sample 432: Loss = 0.4179220497608185, Accuracy = 1.0\n",
            "Sample 433: Loss = 0.4044366776943207, Accuracy = 1.0\n",
            "Sample 434: Loss = 0.8788955807685852, Accuracy = 0.0\n",
            "Sample 435: Loss = 0.29253342747688293, Accuracy = 1.0\n",
            "Sample 436: Loss = 1.9299638271331787, Accuracy = 0.0\n",
            "Sample 437: Loss = 0.38046032190322876, Accuracy = 1.0\n",
            "Sample 438: Loss = 0.5639230608940125, Accuracy = 1.0\n",
            "Sample 439: Loss = 0.1167750209569931, Accuracy = 1.0\n",
            "Sample 440: Loss = 0.9777624011039734, Accuracy = 0.0\n",
            "Sample 441: Loss = 0.9484537839889526, Accuracy = 0.0\n",
            "Sample 442: Loss = 0.18078599870204926, Accuracy = 1.0\n",
            "Sample 443: Loss = 1.7902863025665283, Accuracy = 0.0\n",
            "Sample 444: Loss = 0.2872794270515442, Accuracy = 1.0\n",
            "Sample 445: Loss = 1.6834415197372437, Accuracy = 0.0\n",
            "Sample 446: Loss = 1.0494948625564575, Accuracy = 0.0\n",
            "Sample 447: Loss = 1.0607635974884033, Accuracy = 0.0\n",
            "Sample 448: Loss = 0.0020285521168261766, Accuracy = 1.0\n",
            "Sample 449: Loss = 0.8610494136810303, Accuracy = 0.0\n",
            "Sample 450: Loss = 1.0464903116226196, Accuracy = 0.0\n",
            "Sample 451: Loss = 0.20080673694610596, Accuracy = 1.0\n",
            "Sample 452: Loss = 0.20051461458206177, Accuracy = 1.0\n",
            "Sample 453: Loss = 0.2751772999763489, Accuracy = 1.0\n",
            "Sample 454: Loss = 0.5049205422401428, Accuracy = 1.0\n",
            "Sample 455: Loss = 0.42167001962661743, Accuracy = 1.0\n",
            "Sample 456: Loss = 0.6013638377189636, Accuracy = 1.0\n",
            "Sample 457: Loss = 0.0014121094718575478, Accuracy = 1.0\n",
            "Sample 458: Loss = 0.16219954192638397, Accuracy = 1.0\n",
            "Sample 459: Loss = 0.4683237671852112, Accuracy = 1.0\n",
            "Sample 460: Loss = 0.6151298880577087, Accuracy = 1.0\n",
            "Sample 461: Loss = 0.8542852997779846, Accuracy = 0.0\n",
            "Sample 462: Loss = 0.12093894928693771, Accuracy = 1.0\n",
            "Sample 463: Loss = 0.7897177338600159, Accuracy = 1.0\n",
            "Sample 464: Loss = 0.5711009502410889, Accuracy = 1.0\n",
            "Sample 465: Loss = 0.7870297431945801, Accuracy = 0.0\n",
            "Sample 466: Loss = 0.30723845958709717, Accuracy = 1.0\n",
            "Sample 467: Loss = 0.45835840702056885, Accuracy = 1.0\n",
            "Sample 468: Loss = 0.7731063961982727, Accuracy = 1.0\n",
            "Sample 469: Loss = 0.6068204641342163, Accuracy = 1.0\n",
            "Sample 470: Loss = 0.5103849768638611, Accuracy = 1.0\n",
            "Sample 471: Loss = 0.6418388485908508, Accuracy = 1.0\n",
            "Sample 472: Loss = 0.15334054827690125, Accuracy = 1.0\n",
            "Sample 473: Loss = 0.24374711513519287, Accuracy = 1.0\n",
            "Sample 474: Loss = 1.5097092390060425, Accuracy = 0.0\n",
            "Sample 475: Loss = 0.521118700504303, Accuracy = 1.0\n",
            "Sample 476: Loss = 0.8458311557769775, Accuracy = 0.0\n",
            "Sample 477: Loss = 0.2583200931549072, Accuracy = 1.0\n",
            "Sample 478: Loss = 0.5399001240730286, Accuracy = 1.0\n",
            "Sample 479: Loss = 0.4746644198894501, Accuracy = 1.0\n",
            "Sample 480: Loss = 0.4258313477039337, Accuracy = 1.0\n",
            "Sample 481: Loss = 0.29751458764076233, Accuracy = 1.0\n",
            "Sample 482: Loss = 2.9145660400390625, Accuracy = 0.0\n",
            "Sample 483: Loss = 0.4096970558166504, Accuracy = 1.0\n",
            "Sample 484: Loss = 0.2271752655506134, Accuracy = 1.0\n",
            "Sample 485: Loss = 0.0049241939559578896, Accuracy = 1.0\n",
            "Sample 486: Loss = 1.0697839260101318, Accuracy = 0.0\n",
            "Sample 487: Loss = 0.665235698223114, Accuracy = 1.0\n",
            "Sample 488: Loss = 2.9995169639587402, Accuracy = 0.0\n",
            "Sample 489: Loss = 0.8515374660491943, Accuracy = 0.0\n",
            "Sample 490: Loss = 0.48280999064445496, Accuracy = 1.0\n",
            "Sample 491: Loss = 0.439130961894989, Accuracy = 1.0\n",
            "Sample 492: Loss = 0.8457059860229492, Accuracy = 0.0\n",
            "Sample 493: Loss = 0.08708789944648743, Accuracy = 1.0\n",
            "Sample 494: Loss = 0.8931576609611511, Accuracy = 0.0\n",
            "Sample 495: Loss = 0.7549459338188171, Accuracy = 1.0\n",
            "Sample 496: Loss = 1.0124428272247314, Accuracy = 0.0\n",
            "Sample 497: Loss = 0.30397865176200867, Accuracy = 1.0\n",
            "Sample 498: Loss = 1.6488478183746338, Accuracy = 0.0\n",
            "Sample 499: Loss = 1.718900442123413, Accuracy = 0.0\n",
            "Sample 500: Loss = 0.8214943408966064, Accuracy = 0.0\n",
            "Sample 501: Loss = 0.22797907888889313, Accuracy = 1.0\n",
            "Sample 502: Loss = 0.17671439051628113, Accuracy = 1.0\n",
            "Sample 503: Loss = 0.83794766664505, Accuracy = 1.0\n",
            "Sample 504: Loss = 0.028862951323390007, Accuracy = 1.0\n",
            "Sample 505: Loss = 0.6508837342262268, Accuracy = 1.0\n",
            "Sample 506: Loss = 0.35989782214164734, Accuracy = 1.0\n",
            "Sample 507: Loss = 0.30329713225364685, Accuracy = 1.0\n",
            "Sample 508: Loss = 1.0530604124069214, Accuracy = 0.0\n",
            "Sample 509: Loss = 0.6541408896446228, Accuracy = 1.0\n",
            "Sample 510: Loss = 0.7886962294578552, Accuracy = 0.0\n",
            "Sample 511: Loss = 1.1847598552703857, Accuracy = 0.0\n",
            "Sample 512: Loss = 0.18407411873340607, Accuracy = 1.0\n",
            "Sample 513: Loss = 0.12601067125797272, Accuracy = 1.0\n",
            "Sample 514: Loss = 7.390948667307384e-06, Accuracy = 1.0\n",
            "Sample 515: Loss = 0.21487466990947723, Accuracy = 1.0\n",
            "Sample 516: Loss = 0.4477505683898926, Accuracy = 1.0\n",
            "Sample 517: Loss = 0.29072633385658264, Accuracy = 1.0\n",
            "Sample 518: Loss = 1.7751399278640747, Accuracy = 0.0\n",
            "Sample 519: Loss = 0.46352145075798035, Accuracy = 1.0\n",
            "Sample 520: Loss = 0.644817054271698, Accuracy = 1.0\n",
            "Sample 521: Loss = 0.5984724760055542, Accuracy = 1.0\n",
            "Sample 522: Loss = 0.767680823802948, Accuracy = 0.0\n",
            "Sample 523: Loss = 0.1308901011943817, Accuracy = 1.0\n",
            "Sample 524: Loss = 0.023047544062137604, Accuracy = 1.0\n",
            "Sample 525: Loss = 0.9537741541862488, Accuracy = 0.0\n",
            "Sample 526: Loss = 0.0002989322238136083, Accuracy = 1.0\n",
            "Sample 527: Loss = 0.9521707892417908, Accuracy = 0.0\n",
            "Sample 528: Loss = 0.6152359843254089, Accuracy = 1.0\n",
            "Sample 529: Loss = 0.27931347489356995, Accuracy = 1.0\n",
            "Sample 530: Loss = 0.5513080954551697, Accuracy = 1.0\n",
            "Sample 531: Loss = 0.33358249068260193, Accuracy = 1.0\n",
            "Sample 532: Loss = 0.183290496468544, Accuracy = 1.0\n",
            "Sample 533: Loss = 0.7423790693283081, Accuracy = 1.0\n",
            "Sample 534: Loss = 0.38968348503112793, Accuracy = 1.0\n",
            "Sample 535: Loss = 0.23463541269302368, Accuracy = 1.0\n",
            "Sample 536: Loss = 0.39065951108932495, Accuracy = 1.0\n",
            "Sample 537: Loss = 1.1909371614456177, Accuracy = 0.0\n",
            "Sample 538: Loss = 0.024069948121905327, Accuracy = 1.0\n",
            "Sample 539: Loss = 0.30568671226501465, Accuracy = 1.0\n",
            "Sample 540: Loss = 0.8510570526123047, Accuracy = 0.0\n",
            "Sample 541: Loss = 0.8231542110443115, Accuracy = 1.0\n",
            "Sample 542: Loss = 0.2043810486793518, Accuracy = 1.0\n",
            "Sample 543: Loss = 0.3157702088356018, Accuracy = 1.0\n",
            "Sample 544: Loss = 0.6118581891059875, Accuracy = 1.0\n",
            "Sample 545: Loss = 0.5173583030700684, Accuracy = 1.0\n",
            "Sample 546: Loss = 2.9670403003692627, Accuracy = 0.0\n",
            "Sample 547: Loss = 0.8600314855575562, Accuracy = 0.0\n",
            "Sample 548: Loss = 0.33317986130714417, Accuracy = 1.0\n",
            "Sample 549: Loss = 0.7463502287864685, Accuracy = 1.0\n",
            "Sample 550: Loss = 2.725984573364258, Accuracy = 0.0\n",
            "Sample 551: Loss = 0.5136739611625671, Accuracy = 1.0\n",
            "Sample 552: Loss = 0.7014976143836975, Accuracy = 1.0\n",
            "Sample 553: Loss = 0.21012486517429352, Accuracy = 1.0\n",
            "Sample 554: Loss = 0.5764586925506592, Accuracy = 1.0\n",
            "Sample 555: Loss = 1.4567744731903076, Accuracy = 0.0\n",
            "Sample 556: Loss = 0.4920192360877991, Accuracy = 1.0\n",
            "Sample 557: Loss = 0.5722311735153198, Accuracy = 1.0\n",
            "Sample 558: Loss = 0.8846288919448853, Accuracy = 0.0\n",
            "Sample 559: Loss = 0.5037049055099487, Accuracy = 1.0\n",
            "Sample 560: Loss = 1.0127663612365723, Accuracy = 0.0\n",
            "Sample 561: Loss = 0.039981722831726074, Accuracy = 1.0\n",
            "Sample 562: Loss = 1.3336817026138306, Accuracy = 0.0\n",
            "Sample 563: Loss = 0.8023754954338074, Accuracy = 1.0\n",
            "Sample 564: Loss = 0.33281680941581726, Accuracy = 1.0\n",
            "Sample 565: Loss = 2.9088404178619385, Accuracy = 0.0\n",
            "Sample 566: Loss = 0.28531503677368164, Accuracy = 1.0\n",
            "Sample 567: Loss = 0.8529614210128784, Accuracy = 0.0\n",
            "Sample 568: Loss = 0.38677847385406494, Accuracy = 1.0\n",
            "Sample 569: Loss = 0.40778985619544983, Accuracy = 1.0\n",
            "Sample 570: Loss = 0.291511207818985, Accuracy = 1.0\n",
            "Sample 571: Loss = 0.5230075716972351, Accuracy = 1.0\n",
            "Sample 572: Loss = 0.4558122158050537, Accuracy = 1.0\n",
            "Sample 573: Loss = 0.9701298475265503, Accuracy = 0.0\n",
            "Sample 574: Loss = 0.28103724122047424, Accuracy = 1.0\n",
            "Sample 575: Loss = 0.7589296102523804, Accuracy = 1.0\n",
            "Sample 576: Loss = 0.3423769772052765, Accuracy = 1.0\n",
            "Sample 577: Loss = 0.28030481934547424, Accuracy = 1.0\n",
            "Sample 578: Loss = 0.5719941258430481, Accuracy = 1.0\n",
            "Sample 579: Loss = 1.663271427154541, Accuracy = 0.0\n",
            "Sample 580: Loss = 0.27168864011764526, Accuracy = 1.0\n",
            "Sample 581: Loss = 0.390347421169281, Accuracy = 1.0\n",
            "Sample 582: Loss = 0.7096316814422607, Accuracy = 1.0\n",
            "Sample 583: Loss = 1.9880990982055664, Accuracy = 0.0\n",
            "Sample 584: Loss = 0.059182412922382355, Accuracy = 1.0\n",
            "Sample 585: Loss = 1.1187160015106201, Accuracy = 0.0\n",
            "Sample 586: Loss = 0.28520703315734863, Accuracy = 1.0\n",
            "Sample 587: Loss = 0.7773480415344238, Accuracy = 1.0\n",
            "Sample 588: Loss = 0.37341299653053284, Accuracy = 1.0\n",
            "Sample 589: Loss = 0.5516970753669739, Accuracy = 1.0\n",
            "Sample 590: Loss = 0.7955065369606018, Accuracy = 0.0\n",
            "Sample 591: Loss = 0.1272444725036621, Accuracy = 1.0\n",
            "Sample 592: Loss = 0.0956183671951294, Accuracy = 1.0\n",
            "Sample 593: Loss = 1.824098825454712, Accuracy = 0.0\n",
            "Sample 594: Loss = 0.23492273688316345, Accuracy = 1.0\n",
            "Sample 595: Loss = 0.7966302633285522, Accuracy = 0.0\n",
            "Sample 596: Loss = 0.5270371437072754, Accuracy = 1.0\n",
            "Sample 597: Loss = 0.4021701514720917, Accuracy = 1.0\n",
            "Sample 598: Loss = 0.3818383514881134, Accuracy = 1.0\n",
            "Sample 599: Loss = 0.23480746150016785, Accuracy = 1.0\n",
            "Sample 600: Loss = 0.047028813511133194, Accuracy = 1.0\n",
            "Sample 601: Loss = 1.5962053537368774, Accuracy = 0.0\n",
            "Sample 602: Loss = 0.32126131653785706, Accuracy = 1.0\n",
            "Sample 603: Loss = 0.7607043981552124, Accuracy = 1.0\n",
            "Sample 604: Loss = 0.35135605931282043, Accuracy = 1.0\n",
            "Sample 605: Loss = 0.022010333836078644, Accuracy = 1.0\n",
            "Sample 606: Loss = 0.12238936126232147, Accuracy = 1.0\n",
            "Sample 607: Loss = 1.4870034456253052, Accuracy = 0.0\n",
            "Sample 608: Loss = 1.1295032501220703, Accuracy = 0.0\n",
            "Sample 609: Loss = 0.5778460502624512, Accuracy = 1.0\n",
            "Sample 610: Loss = 0.16219376027584076, Accuracy = 1.0\n",
            "Sample 611: Loss = 0.6440048217773438, Accuracy = 1.0\n",
            "Sample 612: Loss = 0.5265443921089172, Accuracy = 1.0\n",
            "Sample 613: Loss = 0.5836807489395142, Accuracy = 1.0\n",
            "Sample 614: Loss = 0.41350871324539185, Accuracy = 1.0\n",
            "Sample 615: Loss = 0.01338743232190609, Accuracy = 1.0\n",
            "Sample 616: Loss = 0.18162289261817932, Accuracy = 1.0\n",
            "Sample 617: Loss = 0.8348702788352966, Accuracy = 0.0\n",
            "Sample 618: Loss = 0.3660317361354828, Accuracy = 1.0\n",
            "Sample 619: Loss = 1.0000461339950562, Accuracy = 0.0\n",
            "Sample 620: Loss = 0.34738826751708984, Accuracy = 1.0\n",
            "Sample 621: Loss = 0.4692445993423462, Accuracy = 1.0\n",
            "Sample 622: Loss = 0.19975323975086212, Accuracy = 1.0\n",
            "Sample 623: Loss = 0.9750057458877563, Accuracy = 0.0\n",
            "Sample 624: Loss = 0.7072455286979675, Accuracy = 1.0\n",
            "Sample 625: Loss = 0.5635725259780884, Accuracy = 1.0\n",
            "Sample 626: Loss = 0.49441221356391907, Accuracy = 1.0\n",
            "Sample 627: Loss = 0.22252203524112701, Accuracy = 1.0\n",
            "Sample 628: Loss = 0.5525383353233337, Accuracy = 1.0\n",
            "Sample 629: Loss = 0.1893150359392166, Accuracy = 1.0\n",
            "Sample 630: Loss = 0.7202277183532715, Accuracy = 1.0\n",
            "Sample 631: Loss = 0.0515383705496788, Accuracy = 1.0\n",
            "Sample 632: Loss = 1.706923246383667, Accuracy = 0.0\n",
            "Sample 633: Loss = 0.9460842609405518, Accuracy = 0.0\n",
            "Sample 634: Loss = 0.9987688064575195, Accuracy = 0.0\n",
            "Sample 635: Loss = 0.596962034702301, Accuracy = 1.0\n",
            "Sample 636: Loss = 0.22753168642520905, Accuracy = 1.0\n",
            "Sample 637: Loss = 0.4124380052089691, Accuracy = 1.0\n",
            "Sample 638: Loss = 0.20271418988704681, Accuracy = 1.0\n",
            "Sample 639: Loss = 0.1818949282169342, Accuracy = 1.0\n",
            "Sample 640: Loss = 0.027670128270983696, Accuracy = 1.0\n",
            "Sample 641: Loss = 0.8003770112991333, Accuracy = 0.0\n",
            "Sample 642: Loss = 0.4298584759235382, Accuracy = 1.0\n",
            "Sample 643: Loss = 0.7866423726081848, Accuracy = 1.0\n",
            "Sample 644: Loss = 0.55145663022995, Accuracy = 1.0\n",
            "Sample 645: Loss = 0.5337812304496765, Accuracy = 1.0\n",
            "Sample 646: Loss = 1.8468265533447266, Accuracy = 0.0\n",
            "Sample 647: Loss = 1.1920928244535389e-07, Accuracy = 1.0\n",
            "Sample 648: Loss = 0.5570729374885559, Accuracy = 1.0\n",
            "Sample 649: Loss = 0.4439947307109833, Accuracy = 1.0\n",
            "Sample 650: Loss = 0.7435102462768555, Accuracy = 0.0\n",
            "Sample 651: Loss = 0.6131049394607544, Accuracy = 1.0\n",
            "Sample 652: Loss = 1.4274733066558838, Accuracy = 0.0\n",
            "Sample 653: Loss = 0.41381871700286865, Accuracy = 1.0\n",
            "Sample 654: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 655: Loss = 0.2886825203895569, Accuracy = 1.0\n",
            "Sample 656: Loss = 0.8737859129905701, Accuracy = 0.0\n",
            "Sample 657: Loss = 0.7603532075881958, Accuracy = 1.0\n",
            "Sample 658: Loss = 1.1054491996765137, Accuracy = 0.0\n",
            "Sample 659: Loss = 0.21193797886371613, Accuracy = 1.0\n",
            "Sample 660: Loss = 0.007431600242853165, Accuracy = 1.0\n",
            "Sample 661: Loss = 0.7277176380157471, Accuracy = 0.0\n",
            "Sample 662: Loss = 0.4894518256187439, Accuracy = 1.0\n",
            "Sample 663: Loss = 0.7855855822563171, Accuracy = 0.0\n",
            "Sample 664: Loss = 0.31709980964660645, Accuracy = 1.0\n",
            "Sample 665: Loss = 4.768370445162873e-07, Accuracy = 1.0\n",
            "Sample 666: Loss = 1.0079950094223022, Accuracy = 0.0\n",
            "Sample 667: Loss = 0.22487087547779083, Accuracy = 1.0\n",
            "Sample 668: Loss = 1.0693293809890747, Accuracy = 0.0\n",
            "Sample 669: Loss = 1.2703638076782227, Accuracy = 0.0\n",
            "Sample 670: Loss = 1.2587043046951294, Accuracy = 0.0\n",
            "Sample 671: Loss = 0.0019004157511517406, Accuracy = 1.0\n",
            "Sample 672: Loss = 1.5914249420166016, Accuracy = 0.0\n",
            "Sample 673: Loss = 0.8859471678733826, Accuracy = 0.0\n",
            "Sample 674: Loss = 0.21575134992599487, Accuracy = 1.0\n",
            "Sample 675: Loss = 0.007730212062597275, Accuracy = 1.0\n",
            "Sample 676: Loss = 0.25629931688308716, Accuracy = 1.0\n",
            "Sample 677: Loss = 1.8675494194030762, Accuracy = 0.0\n",
            "Sample 678: Loss = 1.0518879890441895, Accuracy = 0.0\n",
            "Sample 679: Loss = 1.3322935104370117, Accuracy = 0.0\n",
            "Sample 680: Loss = 0.23002736270427704, Accuracy = 1.0\n",
            "Sample 681: Loss = 0.3084351420402527, Accuracy = 1.0\n",
            "Sample 682: Loss = 0.39160382747650146, Accuracy = 1.0\n",
            "Sample 683: Loss = 0.9659620523452759, Accuracy = 0.0\n",
            "Sample 684: Loss = 1.7970186471939087, Accuracy = 0.0\n",
            "Sample 685: Loss = 0.8185445666313171, Accuracy = 0.0\n",
            "Sample 686: Loss = 0.4493612051010132, Accuracy = 1.0\n",
            "Sample 687: Loss = 0.4843183159828186, Accuracy = 1.0\n",
            "Sample 688: Loss = 0.4376722276210785, Accuracy = 1.0\n",
            "Sample 689: Loss = 0.4396253228187561, Accuracy = 1.0\n",
            "Sample 690: Loss = 0.0001045410826918669, Accuracy = 1.0\n",
            "Sample 691: Loss = 0.7031566500663757, Accuracy = 1.0\n",
            "Sample 692: Loss = 3.846855640411377, Accuracy = 0.0\n",
            "Sample 693: Loss = 0.42943090200424194, Accuracy = 1.0\n",
            "Sample 694: Loss = 0.04657730460166931, Accuracy = 1.0\n",
            "Sample 695: Loss = 0.17222242057323456, Accuracy = 1.0\n",
            "Sample 696: Loss = 0.5741793513298035, Accuracy = 1.0\n",
            "Sample 697: Loss = 0.7743036150932312, Accuracy = 0.0\n",
            "Sample 698: Loss = 0.33628928661346436, Accuracy = 1.0\n",
            "Sample 699: Loss = 0.7095124125480652, Accuracy = 1.0\n",
            "Sample 700: Loss = 0.9437451362609863, Accuracy = 0.0\n",
            "Sample 701: Loss = 0.3737357556819916, Accuracy = 1.0\n",
            "Sample 702: Loss = 0.5723041296005249, Accuracy = 1.0\n",
            "Sample 703: Loss = 0.769524097442627, Accuracy = 0.0\n",
            "Sample 704: Loss = 0.4391252100467682, Accuracy = 1.0\n",
            "Sample 705: Loss = 0.03287239000201225, Accuracy = 1.0\n",
            "Sample 706: Loss = 0.30969473719596863, Accuracy = 1.0\n",
            "Sample 707: Loss = 0.03578050807118416, Accuracy = 1.0\n",
            "Sample 708: Loss = 0.45178472995758057, Accuracy = 1.0\n",
            "Sample 709: Loss = 0.13289892673492432, Accuracy = 1.0\n",
            "Sample 710: Loss = 0.35567978024482727, Accuracy = 1.0\n",
            "Sample 711: Loss = 0.9804145097732544, Accuracy = 0.0\n",
            "Sample 712: Loss = 0.7006056308746338, Accuracy = 1.0\n",
            "Sample 713: Loss = 0.3648638427257538, Accuracy = 1.0\n",
            "Sample 714: Loss = 0.43457943201065063, Accuracy = 1.0\n",
            "Sample 715: Loss = 0.42478078603744507, Accuracy = 1.0\n",
            "Sample 716: Loss = 0.41369378566741943, Accuracy = 1.0\n",
            "Sample 717: Loss = 0.32617393136024475, Accuracy = 1.0\n",
            "Sample 718: Loss = 0.41714951395988464, Accuracy = 1.0\n",
            "Sample 719: Loss = 0.3047090768814087, Accuracy = 1.0\n",
            "Sample 720: Loss = 0.02281522937119007, Accuracy = 1.0\n",
            "Sample 721: Loss = 0.9976246356964111, Accuracy = 0.0\n",
            "Sample 722: Loss = 0.3424502909183502, Accuracy = 1.0\n",
            "Sample 723: Loss = 1.0122973918914795, Accuracy = 0.0\n",
            "Sample 724: Loss = 1.5762252807617188, Accuracy = 0.0\n",
            "Sample 725: Loss = 0.05926252156496048, Accuracy = 1.0\n",
            "Sample 726: Loss = 0.2065107673406601, Accuracy = 1.0\n",
            "Sample 727: Loss = 0.3004690706729889, Accuracy = 1.0\n",
            "Sample 728: Loss = 0.7978237867355347, Accuracy = 0.0\n",
            "Sample 729: Loss = 0.7645062208175659, Accuracy = 0.0\n",
            "Sample 730: Loss = 0.8329625129699707, Accuracy = 1.0\n",
            "Sample 731: Loss = 0.5429731011390686, Accuracy = 1.0\n",
            "Sample 732: Loss = 1.074265956878662, Accuracy = 0.0\n",
            "Sample 733: Loss = 0.21241545677185059, Accuracy = 1.0\n",
            "Sample 734: Loss = 0.23949258029460907, Accuracy = 1.0\n",
            "Sample 735: Loss = 0.5870394706726074, Accuracy = 1.0\n",
            "Sample 736: Loss = 0.5184341073036194, Accuracy = 1.0\n",
            "Sample 737: Loss = 0.8447009325027466, Accuracy = 0.0\n",
            "Sample 738: Loss = 1.3029024600982666, Accuracy = 0.0\n",
            "Sample 739: Loss = 0.472869336605072, Accuracy = 1.0\n",
            "Sample 740: Loss = 0.08694223314523697, Accuracy = 1.0\n",
            "Sample 741: Loss = 0.8299065828323364, Accuracy = 0.0\n",
            "Sample 742: Loss = 0.27781733870506287, Accuracy = 1.0\n",
            "Sample 743: Loss = 0.702460765838623, Accuracy = 1.0\n",
            "Sample 744: Loss = 0.44745904207229614, Accuracy = 1.0\n",
            "Sample 745: Loss = 0.6281872391700745, Accuracy = 1.0\n",
            "Sample 746: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 747: Loss = 0.6743718385696411, Accuracy = 1.0\n",
            "Sample 748: Loss = 0.1832343190908432, Accuracy = 1.0\n",
            "Sample 749: Loss = 1.7475579977035522, Accuracy = 0.0\n",
            "Sample 750: Loss = 0.22554202377796173, Accuracy = 1.0\n",
            "Sample 751: Loss = 0.07220780849456787, Accuracy = 1.0\n",
            "Sample 752: Loss = 0.3551672101020813, Accuracy = 1.0\n",
            "Sample 753: Loss = 0.4316275715827942, Accuracy = 1.0\n",
            "Sample 754: Loss = 0.577914297580719, Accuracy = 1.0\n",
            "Sample 755: Loss = 0.013599720783531666, Accuracy = 1.0\n",
            "Sample 756: Loss = 1.0876517295837402, Accuracy = 0.0\n",
            "Sample 757: Loss = 0.19481493532657623, Accuracy = 1.0\n",
            "Sample 758: Loss = 0.09727760404348373, Accuracy = 1.0\n",
            "Sample 759: Loss = 0.43035203218460083, Accuracy = 1.0\n",
            "Sample 760: Loss = 0.6861128807067871, Accuracy = 1.0\n",
            "Sample 761: Loss = 0.017116444185376167, Accuracy = 1.0\n",
            "Sample 762: Loss = 0.5047096014022827, Accuracy = 1.0\n",
            "Sample 763: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 764: Loss = 2.557251214981079, Accuracy = 0.0\n",
            "Sample 765: Loss = 0.803900420665741, Accuracy = 0.0\n",
            "Sample 766: Loss = 0.7818213701248169, Accuracy = 1.0\n",
            "Sample 767: Loss = 0.5698308348655701, Accuracy = 1.0\n",
            "Sample 768: Loss = 0.2959454655647278, Accuracy = 1.0\n",
            "Sample 769: Loss = 0.46581122279167175, Accuracy = 1.0\n",
            "Sample 770: Loss = 0.016477692872285843, Accuracy = 1.0\n",
            "Sample 771: Loss = 0.17118166387081146, Accuracy = 1.0\n",
            "Sample 772: Loss = 1.2937557697296143, Accuracy = 0.0\n",
            "Sample 773: Loss = 1.5907268524169922, Accuracy = 0.0\n",
            "Sample 774: Loss = 1.2144709825515747, Accuracy = 0.0\n",
            "Sample 775: Loss = 1.6062101125717163, Accuracy = 0.0\n",
            "Sample 776: Loss = 0.24176795780658722, Accuracy = 1.0\n",
            "Sample 777: Loss = 0.388067364692688, Accuracy = 1.0\n",
            "Sample 778: Loss = 0.6920854449272156, Accuracy = 1.0\n",
            "Sample 779: Loss = 0.06930477172136307, Accuracy = 1.0\n",
            "Sample 780: Loss = 1.0693190097808838, Accuracy = 0.0\n",
            "Sample 781: Loss = 4.286346912384033, Accuracy = 0.0\n",
            "Sample 782: Loss = 0.8034423589706421, Accuracy = 0.0\n",
            "Sample 783: Loss = 1.6869196891784668, Accuracy = 0.0\n",
            "Sample 784: Loss = 1.5497195136049413e-06, Accuracy = 1.0\n",
            "Sample 785: Loss = 0.5390026569366455, Accuracy = 1.0\n",
            "Sample 786: Loss = 1.2239186763763428, Accuracy = 0.0\n",
            "Sample 787: Loss = 0.4750017821788788, Accuracy = 1.0\n",
            "Sample 788: Loss = 0.16736416518688202, Accuracy = 1.0\n",
            "Sample 789: Loss = 0.2153264284133911, Accuracy = 1.0\n",
            "Sample 790: Loss = 0.08263356238603592, Accuracy = 1.0\n",
            "Sample 791: Loss = 0.5605536699295044, Accuracy = 1.0\n",
            "Sample 792: Loss = 0.5022066235542297, Accuracy = 1.0\n",
            "Sample 793: Loss = 0.9238397479057312, Accuracy = 0.0\n",
            "Sample 794: Loss = 1.5169141292572021, Accuracy = 0.0\n",
            "Sample 795: Loss = 0.34825247526168823, Accuracy = 1.0\n",
            "Sample 796: Loss = 0.9716843366622925, Accuracy = 0.0\n",
            "Sample 797: Loss = 1.3124127388000488, Accuracy = 0.0\n",
            "Sample 798: Loss = 0.8508507013320923, Accuracy = 0.0\n",
            "Sample 799: Loss = 0.3293376863002777, Accuracy = 1.0\n",
            "Sample 800: Loss = 1.1421253681182861, Accuracy = 0.0\n",
            "Sample 801: Loss = 0.8567075729370117, Accuracy = 0.0\n",
            "Sample 802: Loss = 0.3722125291824341, Accuracy = 1.0\n",
            "Sample 803: Loss = 0.885913610458374, Accuracy = 1.0\n",
            "Sample 804: Loss = 0.407858282327652, Accuracy = 1.0\n",
            "Sample 805: Loss = 0.1357703059911728, Accuracy = 1.0\n",
            "Sample 806: Loss = 0.562481701374054, Accuracy = 1.0\n",
            "Sample 807: Loss = 0.9902057647705078, Accuracy = 0.0\n",
            "Sample 808: Loss = 0.1263337880373001, Accuracy = 1.0\n",
            "Sample 809: Loss = 1.029545783996582, Accuracy = 0.0\n",
            "Sample 810: Loss = 0.5800380110740662, Accuracy = 1.0\n",
            "Sample 811: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 812: Loss = 0.474895179271698, Accuracy = 1.0\n",
            "Sample 813: Loss = 0.35552191734313965, Accuracy = 1.0\n",
            "Sample 814: Loss = 1.0574417114257812, Accuracy = 0.0\n",
            "Sample 815: Loss = 0.48653990030288696, Accuracy = 1.0\n",
            "Sample 816: Loss = 0.30492255091667175, Accuracy = 1.0\n",
            "Sample 817: Loss = 0.7186391353607178, Accuracy = 1.0\n",
            "Sample 818: Loss = 0.1268806904554367, Accuracy = 1.0\n",
            "Sample 819: Loss = 0.5391600728034973, Accuracy = 1.0\n",
            "Sample 820: Loss = 0.7332591414451599, Accuracy = 1.0\n",
            "Sample 821: Loss = 0.9912141561508179, Accuracy = 0.0\n",
            "Sample 822: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 823: Loss = 0.20352719724178314, Accuracy = 1.0\n",
            "Sample 824: Loss = 0.37841734290122986, Accuracy = 1.0\n",
            "Sample 825: Loss = 0.4773324429988861, Accuracy = 1.0\n",
            "Sample 826: Loss = 1.0444413423538208, Accuracy = 0.0\n",
            "Sample 827: Loss = 0.02033446729183197, Accuracy = 1.0\n",
            "Sample 828: Loss = 0.42445528507232666, Accuracy = 1.0\n",
            "Sample 829: Loss = 0.08825495094060898, Accuracy = 1.0\n",
            "Sample 830: Loss = 0.3831925392150879, Accuracy = 1.0\n",
            "Sample 831: Loss = 0.5468451380729675, Accuracy = 1.0\n",
            "Sample 832: Loss = 0.537043571472168, Accuracy = 1.0\n",
            "Sample 833: Loss = 0.633301317691803, Accuracy = 1.0\n",
            "Sample 834: Loss = 1.0652090311050415, Accuracy = 0.0\n",
            "Sample 835: Loss = 0.16783708333969116, Accuracy = 1.0\n",
            "Sample 836: Loss = 0.4717071056365967, Accuracy = 1.0\n",
            "Sample 837: Loss = 0.4547591805458069, Accuracy = 1.0\n",
            "Sample 838: Loss = 1.1597964763641357, Accuracy = 0.0\n",
            "Sample 839: Loss = 0.14166124165058136, Accuracy = 1.0\n",
            "Sample 840: Loss = 0.41823244094848633, Accuracy = 1.0\n",
            "Sample 841: Loss = 0.5946983098983765, Accuracy = 1.0\n",
            "Sample 842: Loss = 1.0276477336883545, Accuracy = 0.0\n",
            "Sample 843: Loss = 0.8400331139564514, Accuracy = 0.0\n",
            "Sample 844: Loss = 1.3092988729476929, Accuracy = 0.0\n",
            "Sample 845: Loss = 0.7421786189079285, Accuracy = 0.0\n",
            "Sample 846: Loss = 0.8458980917930603, Accuracy = 1.0\n",
            "Sample 847: Loss = 0.1337311714887619, Accuracy = 1.0\n",
            "Sample 848: Loss = 0.28257566690444946, Accuracy = 1.0\n",
            "Sample 849: Loss = 2.3841855067985307e-07, Accuracy = 1.0\n",
            "Sample 850: Loss = 1.0584418773651123, Accuracy = 0.0\n",
            "Sample 851: Loss = 2.3483953555114567e-05, Accuracy = 1.0\n",
            "Sample 852: Loss = 0.2524082660675049, Accuracy = 1.0\n",
            "Sample 853: Loss = 0.3491268455982208, Accuracy = 1.0\n",
            "Sample 854: Loss = 0.9360869526863098, Accuracy = 0.0\n",
            "Sample 855: Loss = 0.5051412582397461, Accuracy = 1.0\n",
            "Sample 856: Loss = 0.9911949634552002, Accuracy = 0.0\n",
            "Sample 857: Loss = 0.4460481107234955, Accuracy = 1.0\n",
            "Sample 858: Loss = 0.1381332278251648, Accuracy = 1.0\n",
            "Sample 859: Loss = 1.9742367267608643, Accuracy = 0.0\n",
            "Sample 860: Loss = 0.2993524372577667, Accuracy = 1.0\n",
            "Sample 861: Loss = 0.6014834642410278, Accuracy = 1.0\n",
            "Sample 862: Loss = 1.1212644577026367, Accuracy = 0.0\n",
            "Sample 863: Loss = 0.7135178446769714, Accuracy = 1.0\n",
            "Sample 864: Loss = 0.2717914581298828, Accuracy = 1.0\n",
            "Sample 865: Loss = 1.348419427871704, Accuracy = 0.0\n",
            "Sample 866: Loss = 0.5622058510780334, Accuracy = 1.0\n",
            "Sample 867: Loss = 0.5247275233268738, Accuracy = 1.0\n",
            "Sample 868: Loss = 0.7914278507232666, Accuracy = 0.0\n",
            "Sample 869: Loss = 0.4104190766811371, Accuracy = 1.0\n",
            "Sample 870: Loss = 0.4661761224269867, Accuracy = 1.0\n",
            "Sample 871: Loss = 3.4274537563323975, Accuracy = 0.0\n",
            "Sample 872: Loss = 0.4860905706882477, Accuracy = 1.0\n",
            "Sample 873: Loss = 1.44966459274292, Accuracy = 0.0\n",
            "Sample 874: Loss = 1.0855753421783447, Accuracy = 0.0\n",
            "Sample 875: Loss = 0.0009934734553098679, Accuracy = 1.0\n",
            "Sample 876: Loss = 0.41716238856315613, Accuracy = 1.0\n",
            "Sample 877: Loss = 0.5067142248153687, Accuracy = 1.0\n",
            "Sample 878: Loss = 0.6720512509346008, Accuracy = 1.0\n",
            "Sample 879: Loss = 0.32346493005752563, Accuracy = 1.0\n",
            "Sample 880: Loss = 0.5075739622116089, Accuracy = 1.0\n",
            "Sample 881: Loss = 0.1903797686100006, Accuracy = 1.0\n",
            "Sample 882: Loss = 0.3857845366001129, Accuracy = 1.0\n",
            "Sample 883: Loss = 0.5922175049781799, Accuracy = 1.0\n",
            "Sample 884: Loss = 0.012827972881495953, Accuracy = 1.0\n",
            "Sample 885: Loss = 0.7034121155738831, Accuracy = 1.0\n",
            "Sample 886: Loss = 0.576403796672821, Accuracy = 1.0\n",
            "Sample 887: Loss = 0.8078189492225647, Accuracy = 1.0\n",
            "Sample 888: Loss = 0.49347782135009766, Accuracy = 1.0\n",
            "Sample 889: Loss = 1.4410288333892822, Accuracy = 0.0\n",
            "Sample 890: Loss = 2.7422423362731934, Accuracy = 0.0\n",
            "Sample 891: Loss = 0.5529124140739441, Accuracy = 1.0\n",
            "Sample 892: Loss = 1.0505415201187134, Accuracy = 1.0\n",
            "Sample 893: Loss = 0.8167829513549805, Accuracy = 0.0\n",
            "Sample 894: Loss = 1.7686917781829834, Accuracy = 0.0\n",
            "Sample 895: Loss = 0.7488797307014465, Accuracy = 1.0\n",
            "Sample 896: Loss = 0.5169929265975952, Accuracy = 1.0\n",
            "Sample 897: Loss = 0.654036283493042, Accuracy = 1.0\n",
            "Sample 898: Loss = 0.45830973982810974, Accuracy = 1.0\n",
            "Sample 899: Loss = 2.9148812294006348, Accuracy = 0.0\n",
            "Sample 900: Loss = 0.9164028167724609, Accuracy = 0.0\n",
            "Sample 901: Loss = 1.167512059211731, Accuracy = 0.0\n",
            "Sample 902: Loss = 0.785007894039154, Accuracy = 1.0\n",
            "Sample 903: Loss = 0.4443059265613556, Accuracy = 1.0\n",
            "Sample 904: Loss = 0.029831413179636, Accuracy = 1.0\n",
            "Sample 905: Loss = 3.2901129722595215, Accuracy = 0.0\n",
            "Sample 906: Loss = 1.0120381116867065, Accuracy = 0.0\n",
            "Sample 907: Loss = 0.7043676376342773, Accuracy = 1.0\n",
            "Sample 908: Loss = 0.944720447063446, Accuracy = 0.0\n",
            "Sample 909: Loss = 1.1203408241271973, Accuracy = 0.0\n",
            "Sample 910: Loss = 0.773651123046875, Accuracy = 1.0\n",
            "Sample 911: Loss = 0.5566783547401428, Accuracy = 1.0\n",
            "Sample 912: Loss = 0.7617843747138977, Accuracy = 0.0\n",
            "Sample 913: Loss = 0.7917265295982361, Accuracy = 0.0\n",
            "Sample 914: Loss = 0.09373094886541367, Accuracy = 1.0\n",
            "Sample 915: Loss = 0.9870958924293518, Accuracy = 0.0\n",
            "Sample 916: Loss = 0.26434481143951416, Accuracy = 1.0\n",
            "Sample 917: Loss = 0.49815988540649414, Accuracy = 1.0\n",
            "Sample 918: Loss = 0.598117470741272, Accuracy = 1.0\n",
            "Sample 919: Loss = 0.30453336238861084, Accuracy = 1.0\n",
            "Sample 920: Loss = 0.9537955522537231, Accuracy = 0.0\n",
            "Sample 921: Loss = 0.45222586393356323, Accuracy = 1.0\n",
            "Sample 922: Loss = 0.37746477127075195, Accuracy = 1.0\n",
            "Sample 923: Loss = 0.43937599658966064, Accuracy = 1.0\n",
            "Sample 924: Loss = 5.471556869451888e-05, Accuracy = 1.0\n",
            "Sample 925: Loss = 0.6650264263153076, Accuracy = 1.0\n",
            "Sample 926: Loss = 0.32993003726005554, Accuracy = 1.0\n",
            "Sample 927: Loss = 0.4768480360507965, Accuracy = 1.0\n",
            "Sample 928: Loss = 0.3675316274166107, Accuracy = 1.0\n",
            "Sample 929: Loss = 0.3568524420261383, Accuracy = 1.0\n",
            "Sample 930: Loss = 0.40217238664627075, Accuracy = 1.0\n",
            "Sample 931: Loss = 0.8699145317077637, Accuracy = 1.0\n",
            "Sample 932: Loss = 2.485671043395996, Accuracy = 0.0\n",
            "Sample 933: Loss = 0.12508845329284668, Accuracy = 1.0\n",
            "Sample 934: Loss = 1.0594749450683594, Accuracy = 0.0\n",
            "Sample 935: Loss = 0.4210708439350128, Accuracy = 1.0\n",
            "Sample 936: Loss = 0.3592281639575958, Accuracy = 1.0\n",
            "Sample 937: Loss = 0.40372225642204285, Accuracy = 1.0\n",
            "Sample 938: Loss = 0.25324851274490356, Accuracy = 1.0\n",
            "Sample 939: Loss = 0.868685245513916, Accuracy = 0.0\n",
            "Sample 940: Loss = 0.0004909025738015771, Accuracy = 1.0\n",
            "Sample 941: Loss = 0.7594895362854004, Accuracy = 0.0\n",
            "Sample 942: Loss = 0.6177695989608765, Accuracy = 1.0\n",
            "Sample 943: Loss = 0.8326664566993713, Accuracy = 0.0\n",
            "Sample 944: Loss = 0.7203211188316345, Accuracy = 1.0\n",
            "Sample 945: Loss = 0.9592037200927734, Accuracy = 0.0\n",
            "Sample 946: Loss = 1.0665264129638672, Accuracy = 0.0\n",
            "Sample 947: Loss = 0.6941261291503906, Accuracy = 1.0\n",
            "Sample 948: Loss = 0.6403334140777588, Accuracy = 1.0\n",
            "Sample 949: Loss = 1.5479788780212402, Accuracy = 0.0\n",
            "Sample 950: Loss = 0.2772075831890106, Accuracy = 1.0\n",
            "Sample 951: Loss = 0.8936997652053833, Accuracy = 0.0\n",
            "Sample 952: Loss = 0.15467162430286407, Accuracy = 1.0\n",
            "Sample 953: Loss = 0.9709129929542542, Accuracy = 0.0\n",
            "Sample 954: Loss = 1.0110446214675903, Accuracy = 0.0\n",
            "Sample 955: Loss = 0.7921631336212158, Accuracy = 1.0\n",
            "Sample 956: Loss = 0.009121650829911232, Accuracy = 1.0\n",
            "Sample 957: Loss = 0.4377289414405823, Accuracy = 1.0\n",
            "Sample 958: Loss = 0.45867353677749634, Accuracy = 1.0\n",
            "Sample 959: Loss = 1.308181881904602, Accuracy = 0.0\n",
            "Sample 960: Loss = 0.2377568781375885, Accuracy = 1.0\n",
            "Sample 961: Loss = 0.7890224456787109, Accuracy = 0.0\n",
            "Sample 962: Loss = 0.9727067351341248, Accuracy = 0.0\n",
            "Sample 963: Loss = 0.8273857235908508, Accuracy = 0.0\n",
            "Sample 964: Loss = 0.9330950975418091, Accuracy = 0.0\n",
            "Sample 965: Loss = 1.6837810277938843, Accuracy = 0.0\n",
            "Sample 966: Loss = 0.8542807102203369, Accuracy = 0.0\n",
            "Sample 967: Loss = 0.06692679971456528, Accuracy = 1.0\n",
            "Sample 968: Loss = 0.5735614895820618, Accuracy = 1.0\n",
            "Sample 969: Loss = 0.6544327735900879, Accuracy = 1.0\n",
            "Sample 970: Loss = 0.47118163108825684, Accuracy = 1.0\n",
            "Sample 971: Loss = 0.43352118134498596, Accuracy = 1.0\n",
            "Sample 972: Loss = 0.8427518606185913, Accuracy = 0.0\n",
            "Sample 973: Loss = 0.40488359332084656, Accuracy = 1.0\n",
            "Sample 974: Loss = 0.5094037055969238, Accuracy = 1.0\n",
            "Sample 975: Loss = 0.7162513136863708, Accuracy = 1.0\n",
            "Sample 976: Loss = 0.09447690844535828, Accuracy = 1.0\n",
            "Sample 977: Loss = 0.5658918023109436, Accuracy = 1.0\n",
            "Sample 978: Loss = 0.24640733003616333, Accuracy = 1.0\n",
            "Sample 979: Loss = 0.537106454372406, Accuracy = 1.0\n",
            "Sample 980: Loss = 0.11430572718381882, Accuracy = 1.0\n",
            "Sample 981: Loss = 0.4031916558742523, Accuracy = 1.0\n",
            "Sample 982: Loss = 0.9885061979293823, Accuracy = 0.0\n",
            "Sample 983: Loss = 1.045041799545288, Accuracy = 0.0\n",
            "Sample 984: Loss = 0.38370612263679504, Accuracy = 1.0\n",
            "Sample 985: Loss = 1.174144983291626, Accuracy = 0.0\n",
            "Sample 986: Loss = 0.4418679475784302, Accuracy = 1.0\n",
            "Sample 987: Loss = 0.2084914892911911, Accuracy = 1.0\n",
            "Sample 988: Loss = 0.7632013559341431, Accuracy = 1.0\n",
            "Sample 989: Loss = 1.4781842764932662e-05, Accuracy = 1.0\n",
            "Sample 990: Loss = 0.4309448003768921, Accuracy = 1.0\n",
            "Sample 991: Loss = 0.9626479744911194, Accuracy = 0.0\n",
            "Sample 992: Loss = 0.6091484427452087, Accuracy = 1.0\n",
            "Sample 993: Loss = 0.21388229727745056, Accuracy = 1.0\n",
            "Sample 994: Loss = 0.8065024614334106, Accuracy = 0.0\n",
            "Sample 995: Loss = 0.44157785177230835, Accuracy = 1.0\n",
            "Sample 996: Loss = 0.271180123090744, Accuracy = 1.0\n",
            "Sample 997: Loss = 0.5807482600212097, Accuracy = 1.0\n",
            "Sample 998: Loss = 0.5630151033401489, Accuracy = 1.0\n",
            "Sample 999: Loss = 0.492689847946167, Accuracy = 1.0\n",
            "Sample 1000: Loss = 0.007201901637017727, Accuracy = 1.0\n",
            "Sample 1001: Loss = 1.1308653354644775, Accuracy = 0.0\n",
            "Sample 1002: Loss = 0.4988301992416382, Accuracy = 1.0\n",
            "Sample 1003: Loss = 3.3253700733184814, Accuracy = 0.0\n",
            "Sample 1004: Loss = 0.7402913570404053, Accuracy = 0.0\n",
            "Sample 1005: Loss = 0.8972142338752747, Accuracy = 0.0\n",
            "Sample 1006: Loss = 0.33720526099205017, Accuracy = 1.0\n",
            "Sample 1007: Loss = 0.09990975260734558, Accuracy = 1.0\n",
            "Sample 1008: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 1009: Loss = 0.7003187537193298, Accuracy = 1.0\n",
            "Sample 1010: Loss = 0.5124048590660095, Accuracy = 1.0\n",
            "Sample 1011: Loss = 0.5535300374031067, Accuracy = 1.0\n",
            "Sample 1012: Loss = 0.7297677397727966, Accuracy = 0.0\n",
            "Sample 1013: Loss = 0.37865084409713745, Accuracy = 1.0\n",
            "Sample 1014: Loss = 1.9366655349731445, Accuracy = 0.0\n",
            "Sample 1015: Loss = 0.3787766396999359, Accuracy = 1.0\n",
            "Sample 1016: Loss = 0.5681300163269043, Accuracy = 1.0\n",
            "Sample 1017: Loss = 0.045618556439876556, Accuracy = 1.0\n",
            "Sample 1018: Loss = 0.8139109015464783, Accuracy = 0.0\n",
            "Sample 1019: Loss = 2.4436914920806885, Accuracy = 0.0\n",
            "Sample 1020: Loss = 0.25347900390625, Accuracy = 1.0\n",
            "Sample 1021: Loss = 0.6568480730056763, Accuracy = 1.0\n",
            "Sample 1022: Loss = 1.0329293012619019, Accuracy = 0.0\n",
            "Sample 1023: Loss = 1.1063178777694702, Accuracy = 0.0\n",
            "Sample 1024: Loss = 0.8253448605537415, Accuracy = 1.0\n",
            "Sample 1025: Loss = 0.41132983565330505, Accuracy = 1.0\n",
            "Sample 1026: Loss = 0.28823402523994446, Accuracy = 1.0\n",
            "Sample 1027: Loss = 0.8311669826507568, Accuracy = 0.0\n",
            "Sample 1028: Loss = 0.1228133887052536, Accuracy = 1.0\n",
            "Sample 1029: Loss = 0.7057955861091614, Accuracy = 0.0\n",
            "Sample 1030: Loss = 0.0002628219372127205, Accuracy = 1.0\n",
            "Sample 1031: Loss = 0.00045265440712682903, Accuracy = 1.0\n",
            "Sample 1032: Loss = 0.6452860832214355, Accuracy = 1.0\n",
            "Sample 1033: Loss = 0.011500963941216469, Accuracy = 1.0\n",
            "Sample 1034: Loss = 0.06375908851623535, Accuracy = 1.0\n",
            "Sample 1035: Loss = 0.3351265490055084, Accuracy = 1.0\n",
            "Sample 1036: Loss = 0.3296445906162262, Accuracy = 1.0\n",
            "Sample 1037: Loss = 0.17281170189380646, Accuracy = 1.0\n",
            "Sample 1038: Loss = 0.5651257038116455, Accuracy = 1.0\n",
            "Sample 1039: Loss = 0.29805219173431396, Accuracy = 1.0\n",
            "Sample 1040: Loss = 1.0356991291046143, Accuracy = 0.0\n",
            "Sample 1041: Loss = 0.8553394675254822, Accuracy = 0.0\n",
            "Sample 1042: Loss = 1.3798024654388428, Accuracy = 0.0\n",
            "Sample 1043: Loss = 0.2523936331272125, Accuracy = 1.0\n",
            "Sample 1044: Loss = 0.9246232509613037, Accuracy = 0.0\n",
            "Sample 1045: Loss = 0.8997350931167603, Accuracy = 0.0\n",
            "Sample 1046: Loss = 0.7274496555328369, Accuracy = 1.0\n",
            "Sample 1047: Loss = 1.485325813293457, Accuracy = 0.0\n",
            "Sample 1048: Loss = 0.9043630957603455, Accuracy = 0.0\n",
            "Sample 1049: Loss = 0.7750470042228699, Accuracy = 1.0\n",
            "Sample 1050: Loss = 0.6193519830703735, Accuracy = 1.0\n",
            "Sample 1051: Loss = 0.7152899503707886, Accuracy = 1.0\n",
            "Sample 1052: Loss = 0.9079939126968384, Accuracy = 0.0\n",
            "Sample 1053: Loss = 0.5447283387184143, Accuracy = 1.0\n",
            "Sample 1054: Loss = 0.8358747363090515, Accuracy = 0.0\n",
            "Sample 1055: Loss = 0.6847870349884033, Accuracy = 1.0\n",
            "Sample 1056: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 1057: Loss = 0.3533853590488434, Accuracy = 1.0\n",
            "Sample 1058: Loss = 0.4683632552623749, Accuracy = 1.0\n",
            "Sample 1059: Loss = 0.23254072666168213, Accuracy = 1.0\n",
            "Sample 1060: Loss = 0.5564919710159302, Accuracy = 1.0\n",
            "Sample 1061: Loss = 0.8459442853927612, Accuracy = 0.0\n",
            "Sample 1062: Loss = 0.9607033729553223, Accuracy = 0.0\n",
            "Sample 1063: Loss = 0.6693087220191956, Accuracy = 1.0\n",
            "Sample 1064: Loss = 0.3655712306499481, Accuracy = 1.0\n",
            "Sample 1065: Loss = 0.7911985516548157, Accuracy = 1.0\n",
            "Sample 1066: Loss = 0.2630082070827484, Accuracy = 1.0\n",
            "Sample 1067: Loss = 0.7167353630065918, Accuracy = 0.0\n",
            "Sample 1068: Loss = 1.1962205171585083, Accuracy = 0.0\n",
            "Sample 1069: Loss = 0.3233122229576111, Accuracy = 1.0\n",
            "Sample 1070: Loss = 1.2059242725372314, Accuracy = 0.0\n",
            "Sample 1071: Loss = 0.5195642709732056, Accuracy = 1.0\n",
            "Sample 1072: Loss = 0.3812657296657562, Accuracy = 1.0\n",
            "Sample 1073: Loss = 1.2477139234542847, Accuracy = 0.0\n",
            "Sample 1074: Loss = 0.8754290342330933, Accuracy = 0.0\n",
            "Sample 1075: Loss = 0.3020726144313812, Accuracy = 1.0\n",
            "Sample 1076: Loss = 0.2650737464427948, Accuracy = 1.0\n",
            "Sample 1077: Loss = 0.6183081865310669, Accuracy = 1.0\n",
            "Sample 1078: Loss = 0.2931400537490845, Accuracy = 1.0\n",
            "Sample 1079: Loss = 0.5116537809371948, Accuracy = 1.0\n",
            "Sample 1080: Loss = 1.033307671546936, Accuracy = 0.0\n",
            "Sample 1081: Loss = 0.8282458782196045, Accuracy = 0.0\n",
            "Sample 1082: Loss = 1.8111355304718018, Accuracy = 0.0\n",
            "Sample 1083: Loss = 0.5078502297401428, Accuracy = 1.0\n",
            "Sample 1084: Loss = 0.8359232544898987, Accuracy = 0.0\n",
            "Sample 1085: Loss = 0.7990380525588989, Accuracy = 1.0\n",
            "Sample 1086: Loss = 0.23550842702388763, Accuracy = 1.0\n",
            "Sample 1087: Loss = 0.9768843054771423, Accuracy = 0.0\n",
            "Sample 1088: Loss = 2.454970359802246, Accuracy = 0.0\n",
            "Sample 1089: Loss = 0.6783743500709534, Accuracy = 1.0\n",
            "Sample 1090: Loss = 0.8819568157196045, Accuracy = 0.0\n",
            "Sample 1091: Loss = 0.3659502863883972, Accuracy = 1.0\n",
            "Sample 1092: Loss = 0.30942532420158386, Accuracy = 1.0\n",
            "Sample 1093: Loss = 0.642810046672821, Accuracy = 1.0\n",
            "Sample 1094: Loss = 0.4999476969242096, Accuracy = 1.0\n",
            "Sample 1095: Loss = 0.008347856812179089, Accuracy = 1.0\n",
            "Sample 1096: Loss = 0.0944710522890091, Accuracy = 1.0\n",
            "Sample 1097: Loss = 0.6267120838165283, Accuracy = 1.0\n",
            "Sample 1098: Loss = 0.10833307355642319, Accuracy = 1.0\n",
            "Sample 1099: Loss = 0.705585777759552, Accuracy = 1.0\n",
            "Sample 1100: Loss = 0.9885048866271973, Accuracy = 0.0\n",
            "Sample 1101: Loss = 0.7918543219566345, Accuracy = 0.0\n",
            "Sample 1102: Loss = 0.09078258275985718, Accuracy = 1.0\n",
            "Sample 1103: Loss = 0.820848822593689, Accuracy = 0.0\n",
            "Sample 1104: Loss = 0.5567399859428406, Accuracy = 1.0\n",
            "Sample 1105: Loss = 0.5587733387947083, Accuracy = 1.0\n",
            "Sample 1106: Loss = 0.4618033170700073, Accuracy = 1.0\n",
            "Sample 1107: Loss = 0.29467764496803284, Accuracy = 1.0\n",
            "Sample 1108: Loss = 0.15780983865261078, Accuracy = 1.0\n",
            "Sample 1109: Loss = 0.678113579750061, Accuracy = 1.0\n",
            "Sample 1110: Loss = 0.04870814457535744, Accuracy = 1.0\n",
            "Sample 1111: Loss = 2.591975688934326, Accuracy = 0.0\n",
            "Sample 1112: Loss = 1.044337511062622, Accuracy = 0.0\n",
            "Sample 1113: Loss = 0.363556444644928, Accuracy = 1.0\n",
            "Sample 1114: Loss = 1.687356948852539, Accuracy = 0.0\n",
            "Sample 1115: Loss = 1.0722155570983887, Accuracy = 0.0\n",
            "Sample 1116: Loss = 0.6003356575965881, Accuracy = 1.0\n",
            "Sample 1117: Loss = 1.9028234481811523, Accuracy = 0.0\n",
            "Sample 1118: Loss = 0.8009704351425171, Accuracy = 0.0\n",
            "Sample 1119: Loss = 0.4169396758079529, Accuracy = 1.0\n",
            "Sample 1120: Loss = 1.5201377868652344, Accuracy = 0.0\n",
            "Sample 1121: Loss = 0.7479114532470703, Accuracy = 0.0\n",
            "Sample 1122: Loss = 0.9418686628341675, Accuracy = 0.0\n",
            "Sample 1123: Loss = 0.3647812306880951, Accuracy = 1.0\n",
            "Sample 1124: Loss = 0.5476076602935791, Accuracy = 1.0\n",
            "Sample 1125: Loss = 0.4306526780128479, Accuracy = 1.0\n",
            "Sample 1126: Loss = 3.975721836090088, Accuracy = 0.0\n",
            "Sample 1127: Loss = 0.7729114890098572, Accuracy = 0.0\n",
            "Sample 1128: Loss = 0.6979794502258301, Accuracy = 1.0\n",
            "Sample 1129: Loss = 0.5256394743919373, Accuracy = 1.0\n",
            "Sample 1130: Loss = 0.8012881875038147, Accuracy = 0.0\n",
            "Sample 1131: Loss = 1.5381062030792236, Accuracy = 0.0\n",
            "Sample 1132: Loss = 0.5993632078170776, Accuracy = 1.0\n",
            "Sample 1133: Loss = 0.6217027902603149, Accuracy = 1.0\n",
            "Sample 1134: Loss = 0.410645455121994, Accuracy = 1.0\n",
            "Sample 1135: Loss = 1.01242196559906, Accuracy = 0.0\n",
            "Sample 1136: Loss = 0.2856137752532959, Accuracy = 1.0\n",
            "Sample 1137: Loss = 0.3671485483646393, Accuracy = 1.0\n",
            "Sample 1138: Loss = 1.1520960330963135, Accuracy = 0.0\n",
            "Sample 1139: Loss = 0.6653299927711487, Accuracy = 1.0\n",
            "Sample 1140: Loss = 2.4749629497528076, Accuracy = 0.0\n",
            "Sample 1141: Loss = 0.22831234335899353, Accuracy = 1.0\n",
            "Sample 1142: Loss = 3.7676453590393066, Accuracy = 0.0\n",
            "Sample 1143: Loss = 0.1537403166294098, Accuracy = 1.0\n",
            "Sample 1144: Loss = 0.3880559504032135, Accuracy = 1.0\n",
            "Sample 1145: Loss = 0.3195430040359497, Accuracy = 1.0\n",
            "Sample 1146: Loss = 0.9247645139694214, Accuracy = 0.0\n",
            "Sample 1147: Loss = 0.32910722494125366, Accuracy = 1.0\n",
            "Sample 1148: Loss = 0.5364484786987305, Accuracy = 1.0\n",
            "Sample 1149: Loss = 0.6716046929359436, Accuracy = 1.0\n",
            "Sample 1150: Loss = 0.3046552836894989, Accuracy = 1.0\n",
            "Sample 1151: Loss = 1.1651501655578613, Accuracy = 0.0\n",
            "Sample 1152: Loss = 0.45142412185668945, Accuracy = 1.0\n",
            "Sample 1153: Loss = 0.1691259741783142, Accuracy = 1.0\n",
            "Sample 1154: Loss = 0.9280728101730347, Accuracy = 0.0\n",
            "Sample 1155: Loss = 0.5646791458129883, Accuracy = 1.0\n",
            "Sample 1156: Loss = 0.14592498540878296, Accuracy = 1.0\n",
            "Sample 1157: Loss = 0.48326197266578674, Accuracy = 1.0\n",
            "Sample 1158: Loss = 0.6183461546897888, Accuracy = 1.0\n",
            "Sample 1159: Loss = 1.4754812717437744, Accuracy = 0.0\n",
            "Sample 1160: Loss = 2.618988275527954, Accuracy = 0.0\n",
            "Sample 1161: Loss = 0.5830546617507935, Accuracy = 1.0\n",
            "Sample 1162: Loss = 0.9494532346725464, Accuracy = 0.0\n",
            "Sample 1163: Loss = 0.19465628266334534, Accuracy = 1.0\n",
            "Sample 1164: Loss = 0.417076051235199, Accuracy = 1.0\n",
            "Sample 1165: Loss = 0.41584834456443787, Accuracy = 1.0\n",
            "Sample 1166: Loss = 0.006397482007741928, Accuracy = 1.0\n",
            "Sample 1167: Loss = 0.3517400622367859, Accuracy = 1.0\n",
            "Sample 1168: Loss = 0.41091132164001465, Accuracy = 1.0\n",
            "Sample 1169: Loss = 1.0231235027313232, Accuracy = 1.0\n",
            "Sample 1170: Loss = 0.48378267884254456, Accuracy = 1.0\n",
            "Sample 1171: Loss = 0.0007264359155669808, Accuracy = 1.0\n",
            "Sample 1172: Loss = 0.59590744972229, Accuracy = 1.0\n",
            "Sample 1173: Loss = 0.36167290806770325, Accuracy = 1.0\n",
            "Sample 1174: Loss = 0.02480841986835003, Accuracy = 1.0\n",
            "Sample 1175: Loss = 0.44762852787971497, Accuracy = 1.0\n",
            "Sample 1176: Loss = 0.3685799837112427, Accuracy = 1.0\n",
            "Sample 1177: Loss = 0.6641757488250732, Accuracy = 1.0\n",
            "Sample 1178: Loss = 0.6729621291160583, Accuracy = 1.0\n",
            "Sample 1179: Loss = 1.3990435600280762, Accuracy = 0.0\n",
            "Sample 1180: Loss = 0.8935120105743408, Accuracy = 0.0\n",
            "Sample 1181: Loss = 0.18603937327861786, Accuracy = 1.0\n",
            "Sample 1182: Loss = 0.5382152199745178, Accuracy = 1.0\n",
            "Sample 1183: Loss = 0.8420644998550415, Accuracy = 0.0\n",
            "Sample 1184: Loss = 0.022669918835163116, Accuracy = 1.0\n",
            "Sample 1185: Loss = 0.5438281297683716, Accuracy = 1.0\n",
            "Sample 1186: Loss = 0.46261367201805115, Accuracy = 1.0\n",
            "Sample 1187: Loss = 0.3416580557823181, Accuracy = 1.0\n",
            "Sample 1188: Loss = 0.10064346343278885, Accuracy = 1.0\n",
            "Sample 1189: Loss = 0.3896186649799347, Accuracy = 1.0\n",
            "Sample 1190: Loss = 1.1650383472442627, Accuracy = 0.0\n",
            "Sample 1191: Loss = 1.0593318939208984, Accuracy = 0.0\n",
            "Sample 1192: Loss = 0.8993293046951294, Accuracy = 0.0\n",
            "Sample 1193: Loss = 0.5601260662078857, Accuracy = 1.0\n",
            "Sample 1194: Loss = 0.13954006135463715, Accuracy = 1.0\n",
            "Sample 1195: Loss = 0.4456942081451416, Accuracy = 1.0\n",
            "Sample 1196: Loss = 0.7918115854263306, Accuracy = 0.0\n",
            "Sample 1197: Loss = 1.5348446369171143, Accuracy = 0.0\n",
            "Sample 1198: Loss = 0.7753646969795227, Accuracy = 1.0\n",
            "Sample 1199: Loss = 1.1659494638442993, Accuracy = 0.0\n",
            "Sample 1200: Loss = 1.2755922079086304, Accuracy = 0.0\n",
            "Sample 1201: Loss = 0.6257668733596802, Accuracy = 1.0\n",
            "Sample 1202: Loss = 0.2401963621377945, Accuracy = 1.0\n",
            "Sample 1203: Loss = 1.3815923929214478, Accuracy = 0.0\n",
            "Sample 1204: Loss = 0.22823463380336761, Accuracy = 1.0\n",
            "Sample 1205: Loss = 0.43989333510398865, Accuracy = 1.0\n",
            "Sample 1206: Loss = 1.1708437204360962, Accuracy = 0.0\n",
            "Sample 1207: Loss = 0.8617154359817505, Accuracy = 0.0\n",
            "Sample 1208: Loss = 0.7574167251586914, Accuracy = 1.0\n",
            "Sample 1209: Loss = 0.5152289271354675, Accuracy = 1.0\n",
            "Sample 1210: Loss = 0.971419632434845, Accuracy = 0.0\n",
            "Sample 1211: Loss = 0.010010396130383015, Accuracy = 1.0\n",
            "Sample 1212: Loss = 1.1166424751281738, Accuracy = 0.0\n",
            "Sample 1213: Loss = 0.6764508485794067, Accuracy = 1.0\n",
            "Sample 1214: Loss = 1.0218806266784668, Accuracy = 0.0\n",
            "Sample 1215: Loss = 3.0550856590270996, Accuracy = 0.0\n",
            "Sample 1216: Loss = 0.5704861283302307, Accuracy = 1.0\n",
            "Sample 1217: Loss = 0.6579819321632385, Accuracy = 1.0\n",
            "Sample 1218: Loss = 0.5441468358039856, Accuracy = 1.0\n",
            "Sample 1219: Loss = 0.7113280892372131, Accuracy = 0.0\n",
            "Sample 1220: Loss = 0.4240660071372986, Accuracy = 1.0\n",
            "Sample 1221: Loss = 1.1365067958831787, Accuracy = 0.0\n",
            "Sample 1222: Loss = 1.2634204626083374, Accuracy = 0.0\n",
            "Sample 1223: Loss = 0.6947485208511353, Accuracy = 1.0\n",
            "Sample 1224: Loss = 0.4014818072319031, Accuracy = 1.0\n",
            "Sample 1225: Loss = 0.04343043640255928, Accuracy = 1.0\n",
            "Sample 1226: Loss = 0.3936452269554138, Accuracy = 1.0\n",
            "Sample 1227: Loss = 0.6418309807777405, Accuracy = 1.0\n",
            "Sample 1228: Loss = 0.44691693782806396, Accuracy = 1.0\n",
            "Sample 1229: Loss = 0.435844361782074, Accuracy = 1.0\n",
            "Sample 1230: Loss = 0.3419320583343506, Accuracy = 1.0\n",
            "Sample 1231: Loss = 0.6685404181480408, Accuracy = 1.0\n",
            "Sample 1232: Loss = 0.7870849967002869, Accuracy = 0.0\n",
            "Sample 1233: Loss = 0.22462989389896393, Accuracy = 1.0\n",
            "Sample 1234: Loss = 3.470515727996826, Accuracy = 0.0\n",
            "Sample 1235: Loss = 0.7189897298812866, Accuracy = 0.0\n",
            "Sample 1236: Loss = 0.5087578296661377, Accuracy = 1.0\n",
            "Sample 1237: Loss = 0.8774577975273132, Accuracy = 0.0\n",
            "Sample 1238: Loss = 0.4098258912563324, Accuracy = 1.0\n",
            "Sample 1239: Loss = 0.7943453192710876, Accuracy = 0.0\n",
            "Sample 1240: Loss = 2.9228076934814453, Accuracy = 0.0\n",
            "Sample 1241: Loss = 0.6802369356155396, Accuracy = 1.0\n",
            "Sample 1242: Loss = 0.5611711144447327, Accuracy = 1.0\n",
            "Sample 1243: Loss = 0.1874481439590454, Accuracy = 1.0\n",
            "Sample 1244: Loss = 0.5008262991905212, Accuracy = 1.0\n",
            "Sample 1245: Loss = 0.7971979975700378, Accuracy = 1.0\n",
            "Sample 1246: Loss = 1.8079724311828613, Accuracy = 0.0\n",
            "Sample 1247: Loss = 0.9278730750083923, Accuracy = 0.0\n",
            "Sample 1248: Loss = 2.276871418871451e-05, Accuracy = 1.0\n",
            "Sample 1249: Loss = 0.9045071601867676, Accuracy = 0.0\n",
            "Sample 1250: Loss = 0.9239455461502075, Accuracy = 0.0\n",
            "Sample 1251: Loss = 0.9831564426422119, Accuracy = 0.0\n",
            "Sample 1252: Loss = 0.006804390344768763, Accuracy = 1.0\n",
            "Sample 1253: Loss = 1.2627551555633545, Accuracy = 0.0\n",
            "Sample 1254: Loss = 0.7659448385238647, Accuracy = 1.0\n",
            "Sample 1255: Loss = 0.0027560130693018436, Accuracy = 1.0\n",
            "Sample 1256: Loss = 0.1761041134595871, Accuracy = 1.0\n",
            "Sample 1257: Loss = 0.28495684266090393, Accuracy = 1.0\n",
            "Sample 1258: Loss = 0.24858403205871582, Accuracy = 1.0\n",
            "Sample 1259: Loss = 0.3986918032169342, Accuracy = 1.0\n",
            "Sample 1260: Loss = 1.0949788093566895, Accuracy = 0.0\n",
            "Sample 1261: Loss = 0.760948121547699, Accuracy = 1.0\n",
            "Sample 1262: Loss = 0.34349313378334045, Accuracy = 1.0\n",
            "Sample 1263: Loss = 1.2289035320281982, Accuracy = 0.0\n",
            "Sample 1264: Loss = 0.6613075733184814, Accuracy = 1.0\n",
            "Sample 1265: Loss = 0.5515425801277161, Accuracy = 1.0\n",
            "Sample 1266: Loss = 0.6488844752311707, Accuracy = 1.0\n",
            "Sample 1267: Loss = 0.47917839884757996, Accuracy = 1.0\n",
            "Sample 1268: Loss = 0.7186780571937561, Accuracy = 0.0\n",
            "Sample 1269: Loss = 0.8775678277015686, Accuracy = 0.0\n",
            "Sample 1270: Loss = 0.5145769119262695, Accuracy = 1.0\n",
            "Sample 1271: Loss = 0.4534270763397217, Accuracy = 1.0\n",
            "Sample 1272: Loss = 0.20863595604896545, Accuracy = 1.0\n",
            "Sample 1273: Loss = 0.32334354519844055, Accuracy = 1.0\n",
            "Sample 1274: Loss = 0.805497944355011, Accuracy = 0.0\n",
            "Sample 1275: Loss = 0.8238260746002197, Accuracy = 0.0\n",
            "Sample 1276: Loss = 0.07685279101133347, Accuracy = 1.0\n",
            "Sample 1277: Loss = 0.8418260216712952, Accuracy = 1.0\n",
            "Sample 1278: Loss = 2.6524996757507324, Accuracy = 0.0\n",
            "Sample 1279: Loss = 0.3342694342136383, Accuracy = 1.0\n",
            "Sample 1280: Loss = 0.3316902220249176, Accuracy = 1.0\n",
            "Sample 1281: Loss = 0.07143083959817886, Accuracy = 1.0\n",
            "Sample 1282: Loss = 0.368497759103775, Accuracy = 1.0\n",
            "Sample 1283: Loss = 0.9551841020584106, Accuracy = 0.0\n",
            "Sample 1284: Loss = 0.746453583240509, Accuracy = 0.0\n",
            "Sample 1285: Loss = 0.3683420717716217, Accuracy = 1.0\n",
            "Sample 1286: Loss = 0.21400703489780426, Accuracy = 1.0\n",
            "Sample 1287: Loss = 1.3572996854782104, Accuracy = 0.0\n",
            "Sample 1288: Loss = 0.05226317420601845, Accuracy = 1.0\n",
            "Sample 1289: Loss = 0.772845447063446, Accuracy = 0.0\n",
            "Sample 1290: Loss = 0.0023529245518147945, Accuracy = 1.0\n",
            "Sample 1291: Loss = 0.3152964115142822, Accuracy = 1.0\n",
            "Sample 1292: Loss = 0.6463236808776855, Accuracy = 1.0\n",
            "Sample 1293: Loss = 0.5005837678909302, Accuracy = 1.0\n",
            "Sample 1294: Loss = 0.4636099338531494, Accuracy = 1.0\n",
            "Sample 1295: Loss = 0.3933259844779968, Accuracy = 1.0\n",
            "Sample 1296: Loss = 0.4370932877063751, Accuracy = 1.0\n",
            "Sample 1297: Loss = 0.36009451746940613, Accuracy = 1.0\n",
            "Sample 1298: Loss = 0.02401292324066162, Accuracy = 1.0\n",
            "Sample 1299: Loss = 1.206002950668335, Accuracy = 0.0\n",
            "Sample 1300: Loss = 1.507235050201416, Accuracy = 0.0\n",
            "Sample 1301: Loss = 0.918569803237915, Accuracy = 0.0\n",
            "Sample 1302: Loss = 0.6546619534492493, Accuracy = 1.0\n",
            "Sample 1303: Loss = 0.7808844447135925, Accuracy = 1.0\n",
            "Sample 1304: Loss = 0.45959168672561646, Accuracy = 1.0\n",
            "Sample 1305: Loss = 0.03622047230601311, Accuracy = 1.0\n",
            "Sample 1306: Loss = 0.9268391132354736, Accuracy = 0.0\n",
            "Sample 1307: Loss = 0.6255173087120056, Accuracy = 1.0\n",
            "Sample 1308: Loss = 0.9893094301223755, Accuracy = 0.0\n",
            "Sample 1309: Loss = 0.4927402436733246, Accuracy = 1.0\n",
            "Sample 1310: Loss = 0.5796079039573669, Accuracy = 1.0\n",
            "Sample 1311: Loss = 2.0700435638427734, Accuracy = 0.0\n",
            "Sample 1312: Loss = 0.6290085911750793, Accuracy = 1.0\n",
            "Sample 1313: Loss = 0.922243058681488, Accuracy = 0.0\n",
            "Sample 1314: Loss = 0.36000168323516846, Accuracy = 1.0\n",
            "Sample 1315: Loss = 1.459122657775879, Accuracy = 0.0\n",
            "Sample 1316: Loss = 0.3302827477455139, Accuracy = 1.0\n",
            "Sample 1317: Loss = 0.1233711838722229, Accuracy = 1.0\n",
            "Sample 1318: Loss = 0.00011872540198964998, Accuracy = 1.0\n",
            "Sample 1319: Loss = 0.7321988344192505, Accuracy = 0.0\n",
            "Sample 1320: Loss = 1.730532169342041, Accuracy = 0.0\n",
            "Sample 1321: Loss = 0.4660181403160095, Accuracy = 1.0\n",
            "Sample 1322: Loss = 0.5495280623435974, Accuracy = 1.0\n",
            "Sample 1323: Loss = 0.305828332901001, Accuracy = 1.0\n",
            "Sample 1324: Loss = 0.7076773643493652, Accuracy = 1.0\n",
            "Sample 1325: Loss = 0.7839246392250061, Accuracy = 0.0\n",
            "Sample 1326: Loss = 3.2741918563842773, Accuracy = 0.0\n",
            "Sample 1327: Loss = 0.10666605085134506, Accuracy = 1.0\n",
            "Sample 1328: Loss = 0.13623322546482086, Accuracy = 1.0\n",
            "Sample 1329: Loss = 0.8222315907478333, Accuracy = 0.0\n",
            "Sample 1330: Loss = 0.46543967723846436, Accuracy = 1.0\n",
            "Sample 1331: Loss = 0.5067490935325623, Accuracy = 1.0\n",
            "Sample 1332: Loss = 0.6281672120094299, Accuracy = 1.0\n",
            "Sample 1333: Loss = 0.1589939445257187, Accuracy = 1.0\n",
            "Sample 1334: Loss = 0.8717988729476929, Accuracy = 0.0\n",
            "Sample 1335: Loss = 0.8147879242897034, Accuracy = 1.0\n",
            "Sample 1336: Loss = 0.9672099351882935, Accuracy = 0.0\n",
            "Sample 1337: Loss = 1.5539894104003906, Accuracy = 0.0\n",
            "Sample 1338: Loss = 0.23854641616344452, Accuracy = 1.0\n",
            "Sample 1339: Loss = 0.40507736802101135, Accuracy = 1.0\n",
            "Sample 1340: Loss = 0.8537136912345886, Accuracy = 0.0\n",
            "Sample 1341: Loss = 0.0005799042410217226, Accuracy = 1.0\n",
            "Sample 1342: Loss = 0.10957902669906616, Accuracy = 1.0\n",
            "Sample 1343: Loss = 1.0170073509216309, Accuracy = 0.0\n",
            "Sample 1344: Loss = 0.08395370095968246, Accuracy = 1.0\n",
            "Sample 1345: Loss = 3.279336452484131, Accuracy = 0.0\n",
            "Sample 1346: Loss = 0.46782025694847107, Accuracy = 1.0\n",
            "Sample 1347: Loss = 0.36306560039520264, Accuracy = 1.0\n",
            "Sample 1348: Loss = 0.19559724628925323, Accuracy = 1.0\n",
            "Sample 1349: Loss = 1.0638763904571533, Accuracy = 0.0\n",
            "Sample 1350: Loss = 0.5639752149581909, Accuracy = 1.0\n",
            "Sample 1351: Loss = 0.3710980713367462, Accuracy = 1.0\n",
            "Sample 1352: Loss = 0.1598002016544342, Accuracy = 1.0\n",
            "Sample 1353: Loss = 0.21579228341579437, Accuracy = 1.0\n",
            "Sample 1354: Loss = 0.062446609139442444, Accuracy = 1.0\n",
            "Sample 1355: Loss = 0.9464426040649414, Accuracy = 0.0\n",
            "Sample 1356: Loss = 0.04005765542387962, Accuracy = 1.0\n",
            "Sample 1357: Loss = 0.4226751923561096, Accuracy = 1.0\n",
            "Sample 1358: Loss = 1.0997570753097534, Accuracy = 0.0\n",
            "Sample 1359: Loss = 0.15912307798862457, Accuracy = 1.0\n",
            "Sample 1360: Loss = 0.8646539449691772, Accuracy = 0.0\n",
            "Sample 1361: Loss = 1.2540035247802734, Accuracy = 0.0\n",
            "Sample 1362: Loss = 0.6657261848449707, Accuracy = 1.0\n",
            "Sample 1363: Loss = 0.11311206966638565, Accuracy = 1.0\n",
            "Sample 1364: Loss = 0.8165420293807983, Accuracy = 0.0\n",
            "Sample 1365: Loss = 0.6922430992126465, Accuracy = 1.0\n",
            "Sample 1366: Loss = 0.9903042912483215, Accuracy = 0.0\n",
            "Sample 1367: Loss = 0.34749844670295715, Accuracy = 1.0\n",
            "Sample 1368: Loss = 0.6598061919212341, Accuracy = 1.0\n",
            "Sample 1369: Loss = 0.591130793094635, Accuracy = 1.0\n",
            "Sample 1370: Loss = 0.4351552724838257, Accuracy = 1.0\n",
            "Sample 1371: Loss = 0.31091058254241943, Accuracy = 1.0\n",
            "Sample 1372: Loss = 1.3585155010223389, Accuracy = 0.0\n",
            "Sample 1373: Loss = 0.38959547877311707, Accuracy = 1.0\n",
            "Sample 1374: Loss = 0.5747167468070984, Accuracy = 1.0\n",
            "Sample 1375: Loss = 0.5800715088844299, Accuracy = 1.0\n",
            "Sample 1376: Loss = 1.5498511791229248, Accuracy = 0.0\n",
            "Sample 1377: Loss = 0.6966625452041626, Accuracy = 1.0\n",
            "Sample 1378: Loss = 0.5330045819282532, Accuracy = 1.0\n",
            "Sample 1379: Loss = 0.711868166923523, Accuracy = 1.0\n",
            "Sample 1380: Loss = 0.5628899335861206, Accuracy = 1.0\n",
            "Sample 1381: Loss = 0.18359939754009247, Accuracy = 1.0\n",
            "Sample 1382: Loss = 0.7102131247520447, Accuracy = 1.0\n",
            "Sample 1383: Loss = 0.9930344820022583, Accuracy = 0.0\n",
            "Sample 1384: Loss = 1.281836986541748, Accuracy = 0.0\n",
            "Sample 1385: Loss = 0.5631970763206482, Accuracy = 1.0\n",
            "Sample 1386: Loss = 0.904739260673523, Accuracy = 0.0\n",
            "Sample 1387: Loss = 0.45574450492858887, Accuracy = 1.0\n",
            "Sample 1388: Loss = 0.41108256578445435, Accuracy = 1.0\n",
            "Sample 1389: Loss = 0.6865479350090027, Accuracy = 1.0\n",
            "Sample 1390: Loss = 0.1365337073802948, Accuracy = 1.0\n",
            "Sample 1391: Loss = 0.8761125206947327, Accuracy = 0.0\n",
            "Sample 1392: Loss = 0.39243283867836, Accuracy = 1.0\n",
            "Sample 1393: Loss = 0.4777475893497467, Accuracy = 1.0\n",
            "Sample 1394: Loss = 0.2362571805715561, Accuracy = 1.0\n",
            "Sample 1395: Loss = 0.6190230846405029, Accuracy = 1.0\n",
            "Sample 1396: Loss = 0.6618293523788452, Accuracy = 1.0\n",
            "Sample 1397: Loss = 0.4654375910758972, Accuracy = 1.0\n",
            "Sample 1398: Loss = 0.6107920408248901, Accuracy = 1.0\n",
            "Sample 1399: Loss = 0.5802255272865295, Accuracy = 1.0\n",
            "Sample 1400: Loss = 0.5512663722038269, Accuracy = 1.0\n",
            "Sample 1401: Loss = 0.018460603430867195, Accuracy = 1.0\n",
            "Sample 1402: Loss = 0.7350878715515137, Accuracy = 0.0\n",
            "Sample 1403: Loss = 0.487087607383728, Accuracy = 1.0\n",
            "Sample 1404: Loss = 1.3888027667999268, Accuracy = 0.0\n",
            "Sample 1405: Loss = 1.242825984954834, Accuracy = 0.0\n",
            "Sample 1406: Loss = 0.7718030214309692, Accuracy = 0.0\n",
            "Sample 1407: Loss = 0.46628090739250183, Accuracy = 1.0\n",
            "Sample 1408: Loss = 0.675946831703186, Accuracy = 1.0\n",
            "Sample 1409: Loss = 0.796571671962738, Accuracy = 1.0\n",
            "Sample 1410: Loss = 0.48378509283065796, Accuracy = 1.0\n",
            "Sample 1411: Loss = 0.836467444896698, Accuracy = 0.0\n",
            "Sample 1412: Loss = 0.008256588131189346, Accuracy = 1.0\n",
            "Sample 1413: Loss = 0.9641035199165344, Accuracy = 0.0\n",
            "Sample 1414: Loss = 1.1615359783172607, Accuracy = 0.0\n",
            "Sample 1415: Loss = 0.27889135479927063, Accuracy = 1.0\n",
            "Sample 1416: Loss = 0.33913615345954895, Accuracy = 1.0\n",
            "Sample 1417: Loss = 0.7553781867027283, Accuracy = 1.0\n",
            "Sample 1418: Loss = 0.4273528754711151, Accuracy = 1.0\n",
            "Sample 1419: Loss = 0.16799329221248627, Accuracy = 1.0\n",
            "Sample 1420: Loss = 1.0712916851043701, Accuracy = 0.0\n",
            "Sample 1421: Loss = 0.5279787182807922, Accuracy = 1.0\n",
            "Sample 1422: Loss = 0.993844747543335, Accuracy = 0.0\n",
            "Sample 1423: Loss = 0.20696426928043365, Accuracy = 1.0\n",
            "Sample 1424: Loss = 0.5492717027664185, Accuracy = 1.0\n",
            "Sample 1425: Loss = 1.0113362073898315, Accuracy = 0.0\n",
            "Sample 1426: Loss = 2.4899251461029053, Accuracy = 0.0\n",
            "Sample 1427: Loss = 0.056129686534404755, Accuracy = 1.0\n",
            "Sample 1428: Loss = 0.2037065327167511, Accuracy = 1.0\n",
            "Sample 1429: Loss = 1.4104669094085693, Accuracy = 0.0\n",
            "Sample 1430: Loss = 0.568964958190918, Accuracy = 1.0\n",
            "Sample 1431: Loss = 0.0, Accuracy = 1.0\n",
            "Sample 1432: Loss = 0.3922944962978363, Accuracy = 1.0\n",
            "Sample 1433: Loss = 0.7375239133834839, Accuracy = 0.0\n",
            "Sample 1434: Loss = 1.43678617477417, Accuracy = 0.0\n",
            "Sample 1435: Loss = 0.8490081429481506, Accuracy = 0.0\n",
            "Sample 1436: Loss = 0.2339683622121811, Accuracy = 1.0\n",
            "Sample 1437: Loss = 0.1402004063129425, Accuracy = 1.0\n",
            "Sample 1438: Loss = 0.39593011140823364, Accuracy = 1.0\n",
            "Sample 1439: Loss = 0.3252564072608948, Accuracy = 1.0\n",
            "Sample 1440: Loss = 0.3927914798259735, Accuracy = 1.0\n",
            "Sample 1441: Loss = 0.5181504487991333, Accuracy = 1.0\n",
            "Sample 1442: Loss = 0.5501044988632202, Accuracy = 1.0\n",
            "Sample 1443: Loss = 0.9077834486961365, Accuracy = 0.0\n",
            "Sample 1444: Loss = 1.026633858680725, Accuracy = 0.0\n",
            "Sample 1445: Loss = 1.3107786178588867, Accuracy = 0.0\n",
            "Sample 1446: Loss = 0.36366716027259827, Accuracy = 1.0\n",
            "Sample 1447: Loss = 0.8321934938430786, Accuracy = 0.0\n",
            "Sample 1448: Loss = 0.5576640367507935, Accuracy = 1.0\n",
            "Sample 1449: Loss = 0.3539932370185852, Accuracy = 1.0\n",
            "Sample 1450: Loss = 0.5156844258308411, Accuracy = 1.0\n",
            "Sample 1451: Loss = 2.0700199604034424, Accuracy = 0.0\n",
            "Sample 1452: Loss = 0.6819549798965454, Accuracy = 1.0\n",
            "Sample 1453: Loss = 0.5653113126754761, Accuracy = 1.0\n",
            "Sample 1454: Loss = 0.00031740395934320986, Accuracy = 1.0\n",
            "Sample 1455: Loss = 0.4185180068016052, Accuracy = 1.0\n",
            "Sample 1456: Loss = 0.8341969847679138, Accuracy = 0.0\n",
            "Sample 1457: Loss = 0.1900637000799179, Accuracy = 1.0\n",
            "Sample 1458: Loss = 0.7370208501815796, Accuracy = 1.0\n",
            "Sample 1459: Loss = 0.28043341636657715, Accuracy = 1.0\n",
            "Sample 1460: Loss = 2.9813311100006104, Accuracy = 0.0\n",
            "Sample 1461: Loss = 0.09958154708147049, Accuracy = 1.0\n",
            "Sample 1462: Loss = 0.5109718441963196, Accuracy = 1.0\n",
            "Sample 1463: Loss = 0.14770929515361786, Accuracy = 1.0\n",
            "Sample 1464: Loss = 0.16884608566761017, Accuracy = 1.0\n",
            "Sample 1465: Loss = 0.27342578768730164, Accuracy = 1.0\n",
            "Sample 1466: Loss = 1.1920928244535389e-07, Accuracy = 1.0\n",
            "Sample 1467: Loss = 0.7904196977615356, Accuracy = 1.0\n",
            "Sample 1468: Loss = 0.03204749524593353, Accuracy = 1.0\n",
            "Sample 1469: Loss = 0.7931452393531799, Accuracy = 0.0\n",
            "Sample 1470: Loss = 0.2646220624446869, Accuracy = 1.0\n",
            "Sample 1471: Loss = 1.708327054977417, Accuracy = 0.0\n",
            "Sample 1472: Loss = 0.34364616870880127, Accuracy = 1.0\n",
            "Sample 1473: Loss = 0.16406849026679993, Accuracy = 1.0\n",
            "Sample 1474: Loss = 1.0690112113952637, Accuracy = 0.0\n",
            "Sample 1475: Loss = 0.7628329992294312, Accuracy = 0.0\n",
            "Sample 1476: Loss = 0.5076562166213989, Accuracy = 1.0\n",
            "Sample 1477: Loss = 0.19048304855823517, Accuracy = 1.0\n",
            "Sample 1478: Loss = 0.09445987641811371, Accuracy = 1.0\n",
            "Sample 1479: Loss = 0.6122730374336243, Accuracy = 1.0\n",
            "Sample 1480: Loss = 0.8899915218353271, Accuracy = 0.0\n",
            "Sample 1481: Loss = 0.2564852833747864, Accuracy = 1.0\n",
            "Sample 1482: Loss = 0.6406514644622803, Accuracy = 1.0\n",
            "Sample 1483: Loss = 0.5986281633377075, Accuracy = 1.0\n",
            "Sample 1484: Loss = 0.044663675129413605, Accuracy = 1.0\n",
            "Sample 1485: Loss = 1.5315465927124023, Accuracy = 0.0\n",
            "Sample 1486: Loss = 0.44063815474510193, Accuracy = 1.0\n",
            "Sample 1487: Loss = 0.2043907642364502, Accuracy = 1.0\n",
            "Sample 1488: Loss = 0.3432281017303467, Accuracy = 1.0\n",
            "Sample 1489: Loss = 0.4854924976825714, Accuracy = 1.0\n",
            "Sample 1490: Loss = 0.674680769443512, Accuracy = 1.0\n",
            "Sample 1491: Loss = 0.03804577514529228, Accuracy = 1.0\n",
            "Sample 1492: Loss = 0.7009134888648987, Accuracy = 1.0\n",
            "Sample 1493: Loss = 1.0243748426437378, Accuracy = 0.0\n",
            "Sample 1494: Loss = 2.957406997680664, Accuracy = 0.0\n",
            "Sample 1495: Loss = 0.7761527299880981, Accuracy = 1.0\n",
            "Sample 1496: Loss = 0.40474188327789307, Accuracy = 1.0\n",
            "Sample 1497: Loss = 1.1920928244535389e-07, Accuracy = 1.0\n",
            "Sample 1498: Loss = 0.6147359013557434, Accuracy = 1.0\n",
            "Sample 1499: Loss = 0.7524831891059875, Accuracy = 1.0\n",
            "Sample 1500: Loss = 0.8644166588783264, Accuracy = 0.0\n",
            "Sample 1501: Loss = 1.2857519388198853, Accuracy = 0.0\n",
            "Sample 1502: Loss = 0.07832234352827072, Accuracy = 1.0\n",
            "Sample 1503: Loss = 1.0022540092468262, Accuracy = 0.0\n",
            "Sample 1504: Loss = 0.5592370629310608, Accuracy = 1.0\n",
            "Sample 1505: Loss = 0.3654952049255371, Accuracy = 1.0\n",
            "Sample 1506: Loss = 0.5399478077888489, Accuracy = 1.0\n",
            "Sample 1507: Loss = 1.2516567707061768, Accuracy = 0.0\n",
            "Sample 1508: Loss = 0.10289160162210464, Accuracy = 1.0\n",
            "Sample 1509: Loss = 0.19551803171634674, Accuracy = 1.0\n",
            "Sample 1510: Loss = 1.4571418762207031, Accuracy = 0.0\n",
            "Sample 1511: Loss = 0.47952988743782043, Accuracy = 1.0\n",
            "Sample 1512: Loss = 2.9328150749206543, Accuracy = 0.0\n",
            "Sample 1513: Loss = 0.5682254433631897, Accuracy = 1.0\n",
            "Sample 1514: Loss = 1.047661304473877, Accuracy = 0.0\n",
            "Sample 1515: Loss = 1.013437271118164, Accuracy = 0.0\n",
            "Sample 1516: Loss = 0.6118118762969971, Accuracy = 1.0\n",
            "Sample 1517: Loss = 0.34350284934043884, Accuracy = 1.0\n",
            "Sample 1518: Loss = 2.2806553840637207, Accuracy = 0.0\n",
            "Sample 1519: Loss = 0.08724042028188705, Accuracy = 1.0\n",
            "Sample 1520: Loss = 0.1534634530544281, Accuracy = 1.0\n",
            "Sample 1521: Loss = 0.008977644145488739, Accuracy = 1.0\n",
            "Sample 1522: Loss = 0.7497943639755249, Accuracy = 1.0\n",
            "Sample 1523: Loss = 0.5702391266822815, Accuracy = 1.0\n",
            "Sample 1524: Loss = 0.43065065145492554, Accuracy = 1.0\n",
            "Sample 1525: Loss = 0.10696795582771301, Accuracy = 1.0\n",
            "Sample 1526: Loss = 0.3496398627758026, Accuracy = 1.0\n",
            "Sample 1527: Loss = 0.8280586004257202, Accuracy = 0.0\n",
            "Sample 1528: Loss = 0.839677095413208, Accuracy = 0.0\n",
            "Sample 1529: Loss = 1.29008948802948, Accuracy = 0.0\n",
            "Sample 1530: Loss = 0.3070950210094452, Accuracy = 1.0\n",
            "Sample 1531: Loss = 0.6685583591461182, Accuracy = 1.0\n",
            "Sample 1532: Loss = 0.3829890191555023, Accuracy = 1.0\n",
            "Sample 1533: Loss = 0.020482221618294716, Accuracy = 1.0\n",
            "Sample 1534: Loss = 0.21144302189350128, Accuracy = 1.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-75b8138f3e2c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Train the model on the current sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Print the results for the current sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialize lists to store evaluation results\n",
        "losses = []\n",
        "accuracies = []\n",
        "correct_predictions = 0\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    # Evaluate the current sample\n",
        "    loss, accuracy = model.evaluate(X_test[i:i+1], y_test.iloc[i:i+1], verbose=0)\n",
        "\n",
        "    # Append the loss and accuracy\n",
        "    losses.append(loss)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Predict the label for the current sample\n",
        "    prediction = model.predict(X_test[i:i+1], verbose=0)\n",
        "\n",
        "    # Check if the prediction is correct\n",
        "    if prediction.argmax() == y_test.iloc[i].argmax():  # Removed extra indent here\n",
        "        correct_predictions += 1\n",
        "\n",
        "    # Train the model on the current sample\n",
        "    model.train_on_batch(X_test[i:i+1], y_test.iloc[i:i+1])\n",
        "\n",
        "    # Print the results for the current sample\n",
        "    print(f\"Sample {i+1}: Loss = {losses[i]}, Accuracy = {accuracies[i]}\")\n",
        "\n",
        "# Calculate the percentage of correct predictions\n",
        "percentage_correct = (correct_predictions / len(X_test)) * 100\n",
        "\n",
        "print(f\"Percentage of correct predictions: {percentage_correct:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzOnCi_X2XbA",
        "outputId": "0758d727-5659-41a4-8a12-ce8b3024e7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m815/815\u001b[0m \u001b[32mโโโโโโโโโโโโโโโโโโโโ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "Confusion Matrix:\n",
            "[[11443    15  2174]\n",
            " [ 2075   364   123]\n",
            " [ 6188     2  3686]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get predictions from the model\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Matriz de confusรฃo original\n",
        "confusion_matrix = np.array([\n",
        "    [38696, 50, 6614],\n",
        "    [5588, 2257, 760],\n",
        "    [18595, 25, 14315]\n",
        "])\n",
        "\n",
        "# Calcular a soma total de todos os elementos da matriz\n",
        "total_sum = np.sum(confusion_matrix)\n",
        "\n",
        "# Converter cada valor para porcentagem\n",
        "confusion_matrix_percent = (confusion_matrix / total_sum) * 100\n",
        "\n",
        "# Arredondar os valores para 2 casas decimais e garantir que nรฃo use notaรงรฃo cientรญfica\n",
        "confusion_matrix_percent_rounded = np.round(confusion_matrix_percent, 2)\n",
        "\n",
        "# Formatar a matriz para exibir os valores corretamente\n",
        "np.set_printoptions(suppress=True, precision=2)\n",
        "\n",
        "print(\"Matriz de Confusรฃo em Porcentagens:\")\n",
        "print(confusion_matrix_percent_rounded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCiQCzt7Rl_F",
        "outputId": "45f57c80-9188-4110-b74c-8304b0993ff1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusรฃo em Porcentagens:\n",
            "[[44.53  0.06  7.61]\n",
            " [ 6.43  2.6   0.87]\n",
            " [21.4   0.03 16.47]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzhBweUUHei8",
        "outputId": "44721ff3-01ac-4bc6-ebf7-c2d9422e2bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('fulldata_porto.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3rfhfEFeILN",
        "outputId": "c3e46109-21ea-4d6a-f659-c3631059ca58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.12.23)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1\n"
          ]
        }
      ],
      "source": [
        "pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "dY2sdW-1HfBp",
        "outputId": "4bfcec75-b9a1-4f56-f8d2-958f4913dad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.26.4)\n",
            "Collecting onnx>=1.4.1 (from tf2onnx)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (24.12.23)\n",
            "Collecting protobuf~=3.20 (from tf2onnx)\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.12.14)\n",
            "Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 protobuf-3.20.3 tf2onnx-1.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "b7b5d1d28ad74797be770a07005d1893"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install tf2onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4KZSrnfHiFh",
        "outputId": "aead9017-76b2-46da-e633-e3f4cc464249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-13 16:21:31.246157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-13 16:21:31.438456: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-13 16:21:31.462032: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-13 16:21:35.154289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2025-01-13 16:21:41,132 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2025-01-13 16:21:41,727 - INFO - Signatures found in model: [serving_default].\n",
            "2025-01-13 16:21:41,729 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2025-01-13 16:21:41,732 - INFO - Output names: ['output_0']\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1736785301.744436    6267 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "I0000 00:00:1736785302.415754    6267 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2025-01-13 16:21:42,663 - INFO - Using tensorflow=2.17.1, onnx=1.17.0, tf2onnx=1.16.1/15c810\n",
            "2025-01-13 16:21:42,663 - INFO - Using opset <onnx, 15>\n",
            "2025-01-13 16:21:42,788 - INFO - Computed 0 values for constant folding\n",
            "2025-01-13 16:21:42,972 - INFO - Optimizing ONNX model\n",
            "2025-01-13 16:21:43,693 - INFO - After optimization: Identity -2 (2->0)\n",
            "2025-01-13 16:21:43,712 - INFO - \n",
            "2025-01-13 16:21:43,712 - INFO - Successfully converted TensorFlow model /content/drive/MyDrive/saved_model to ONNX\n",
            "2025-01-13 16:21:43,712 - INFO - Model inputs: ['inputs']\n",
            "2025-01-13 16:21:43,712 - INFO - Model outputs: ['output_0']\n",
            "2025-01-13 16:21:43,712 - INFO - ONNX model is saved at /content/drive/MyDrive/fulldata_porto.onnx\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Carregar o modelo Keras\n",
        "modelo = tf.keras.models.load_model('/content/fulldata_porto.h5')\n",
        "\n",
        "# Converter o modelo Keras para ONNX\n",
        "tf.saved_model.save(modelo, \"/content/drive/MyDrive/saved_model\")\n",
        "!python -m tf2onnx.convert --saved-model /content/drive/MyDrive/saved_model --output /content/drive/MyDrive/fulldata_porto.onnx\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}